fit_group <- lavaan::sem(model, data = scores_scaled, group = "IDCNTRY")
summary(fit_group, fit.measures = TRUE)
# å°†IDCNTRYåˆ—è½¬æ¢ä¸ºå› å­ç±»å‹
merged_data <- merged_data[!is.na(merged_data$CNTRY), ]
merged_data$CNTRY <- factor(merged_data$CNTRY)
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹
fit_group <- lavaan::sem(model, data = scores_scaled, group = "IDCNTRY")
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹
fit_group <- lavaan::sem(model, data = merged_data, group = "IDCNTRY")
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹
fit_group <- lavaan::sem(model1, data = merged_data, group = "IDCNTRY")
summary(fit_group, fit.measures = TRUE)
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹
fit_group <- lavaan::sem(model1, data = merged_data, group = "IDCNTRY")
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹
fit_group <- lavaan::sem(model1, data = merged_data, group = "CNTRY")
summary(fit_group, fit.measures = TRUE)
# æå–å›å½’ç³»æ•°å’Œå‚æ•°ä¼°è®¡
param_estimates <- lavaan::parameterEstimates(fit_group)
regression_estimates <- param_estimates[param_estimates$op == "~", ]
# æŒ‰ç…§å›½å®¶åˆ†ç»„æŸ¥çœ‹ç»“æœ
split(regression_estimates, regression_estimates$group)
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹2
fit_group2 <- lavaan::sem(model2, data = merged_data, group = "CNTRY")
summary(fit_group2, fit.measures = TRUE)
# æå–å›å½’ç³»æ•°å’Œå‚æ•°ä¼°è®¡
param_estimates2 <- lavaan::parameterEstimates(fit_group2)
regression_estimates2 <- param_estimates2[param_estimates$op == "~", ]
# æŒ‰ç…§å›½å®¶åˆ†ç»„æŸ¥çœ‹ç»“æœ
split(regression_estimates1, regression_estimates1$group)
# æŒ‰ç…§å›½å®¶åˆ†ç»„æŸ¥çœ‹ç»“æœ
split(regression_estimates2, regression_estimates2$group)
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹3
fit_group3 <- lavaan::sem(model3, data = merged_data, group = "CNTRY")
summary(fit_group3, fit.measures = TRUE)
# æå–å›å½’ç³»æ•°å’Œå‚æ•°ä¼°è®¡
param_estimates3 <- lavaan::parameterEstimates(fit_group3)
regression_estimates3 <- param_estimates1[param_estimates$op == "~", ]
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹3
fit_group3 <- lavaan::sem(model3, data = merged_data, group = "CNTRY")
summary(fit_group3, fit.measures = TRUE)
# æå–å›å½’ç³»æ•°å’Œå‚æ•°ä¼°è®¡
param_estimates3 <- lavaan::parameterEstimates(fit_group3)
regression_estimates3 <- param_estimates3[param_estimates$op == "~", ]
# æŒ‰ç…§å›½å®¶åˆ†ç»„æŸ¥çœ‹ç»“æœ
split(regression_estimates3, regression_estimates3$group)
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹4
fit_group4 <- lavaan::sem(model4, data = merged_data, group = "CNTRY")
summary(fit_group4, fit.measures = TRUE)
# æå–å›å½’ç³»æ•°å’Œå‚æ•°ä¼°è®¡
param_estimates4 <- lavaan::parameterEstimates(fit_group4)
regression_estimates4 <- param_estimates4[param_estimates$op == "~", ]
# æŒ‰ç…§å›½å®¶åˆ†ç»„æŸ¥çœ‹ç»“æœ
split(regression_estimates4, regression_estimates4$group)
summary(fit1, fit.measures = TRUE, standardized = TRUE)
# æŒ‰ç…§å›½å®¶åˆ†ç»„æŸ¥çœ‹ç»“æœ
split(regression_estimates3, regression_estimates3$group)
summary(fit1, fit.measures = TRUE, standardized = TRUE)
#æ ¡é•¿å­¦ä¹ é¢†å¯¼åŠ› â†’ ç»„ç»‡æ°›å›´
model1 <- '
# ç»„ç»‡æ°›å›´
T3STUD =~ TT3G49A + TT3G49B + TT3G49C + TT3G49D
T3COLES =~ TT3G33A + TT3G33B + TT3G33C + TT3G33H
# é¢†å¯¼åŠ›å½±å“ç»„ç»‡æ°›å›´
T3STUD ~ T3PLEADS_score
T3COLES ~ T3PLEADS_score
'
summary(fit1, fit.measures = TRUE, standardized = TRUE)
# æŒ‰ç…§å›½å®¶åˆ†ç»„æŸ¥çœ‹ç»“æœ
split(regression_estimates1, regression_estimates1$group)
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹2
fit_group2 <- lavaan::sem(model2, data = merged_data, group = "CNTRY")
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹1
fit_group1 <- lavaan::sem(model1, data = merged_data, group = "CNTRY")
summary(fit_group1, fit.measures = TRUE)
# æå–å›å½’ç³»æ•°å’Œå‚æ•°ä¼°è®¡
param_estimates1 <- lavaan::parameterEstimates(fit_group1)
regression_estimates1 <- param_estimates1[param_estimates$op == "~", ]
# æŒ‰ç…§å›½å®¶åˆ†ç»„æŸ¥çœ‹ç»“æœ
split(regression_estimates1, regression_estimates1$group)
#æ ¡é•¿å­¦ä¹ é¢†å¯¼åŠ› â†’ æ•™å¸ˆæƒ…æ„Ÿ
model2 <- '
# æ•™å¸ˆæƒ…æ„Ÿ
T3TEAM =~ TT3G32A + TT3G32B + TT3G32C + TT3G32D
T3SELF =~ TT3G34A + TT3G34B + TT3G34C + TT3G34D + TT3G34E + TT3G34F + TT3G34G + TT3G34H + TT3G34I + TT3G34J + TT3G34K + TT3G34L
TT3G49E ~~ TT3G49E  # è¿™ä¸ªå˜é‡åªæœ‰ä¸€ä¸ªè§‚æµ‹é‡ï¼Œä¸èƒ½å»ºå› å­ï¼Œç›´æ¥å»ºå›å½’
# é¢†å¯¼åŠ›å½±å“æ•™å¸ˆæƒ…æ„Ÿ
T3TEAM ~ T3PLEADS_score
T3SELF ~ T3PLEADS_score
TT3G49E ~ T3PLEADS_score
'
fit2 <- sem(model2, data = merged_data, missing = "fiml")
summary(fit2, fit.measures = TRUE, standardized = TRUE)
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹2
fit_group2 <- lavaan::sem(model2, data = merged_data, group = "CNTRY")
summary(fit_group2, fit.measures = TRUE)
# æå–å›å½’ç³»æ•°å’Œå‚æ•°ä¼°è®¡
param_estimates2 <- lavaan::parameterEstimates(fit_group2)
regression_estimates2 <- param_estimates2[param_estimates$op == "~", ]
# æŒ‰ç…§å›½å®¶åˆ†ç»„æŸ¥çœ‹ç»“æœ
split(regression_estimates2, regression_estimates2$group)
#æ ¡é•¿å­¦ä¹ é¢†å¯¼åŠ› â†’ è¯¾å ‚æ•™å­¦
model3 <- '
# è¯¾å ‚æ•™å­¦
T3TPRA =~ TT3G42A + TT3G42B + TT3G42C + TT3G42D + TT3G42E + TT3G42F + TT3G42G + TT3G42H + TT3G42I + TT3G42J + TT3G42K + TT3G42L
T3DISC =~ TT3G41A + TT3G41B + TT3G41C + TT3G41D
# é¢†å¯¼åŠ›å½±å“è¯¾å ‚æ•™å­¦
T3TPRA ~ T3PLEADS_score
T3DISC ~ T3PLEADS_score
'
fit3 <- sem(model3, data = merged_data, missing = "fiml")
summary(fit3, fit.measures = TRUE, standardized = TRUE)
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹3
fit_group3 <- lavaan::sem(model3, data = merged_data, group = "CNTRY")
summary(fit_group3, fit.measures = TRUE)
# æå–å›å½’ç³»æ•°å’Œå‚æ•°ä¼°è®¡
param_estimates3 <- lavaan::parameterEstimates(fit_group3)
regression_estimates3 <- param_estimates3[param_estimates$op == "~", ]
# æŒ‰ç…§å›½å®¶åˆ†ç»„æŸ¥çœ‹ç»“æœ
split(regression_estimates3, regression_estimates3$group)
summary(fit4, fit.measures = TRUE, standardized = TRUE)
#å¦‚æœä½ è®¤ä¸ºæ ¡é•¿å­¦ä¹ é¢†å¯¼åŠ› T3PLEADS çš„å½±å“å—å…¶ä»–å› ç´ è°ƒèŠ‚ï¼ˆå¦‚ T3COLES æˆ– T3TEAMï¼‰ï¼Œå¯ä»¥æ·»åŠ äº¤äº’é¡¹ã€‚ä¾‹å¦‚ï¼Œæ•™å¸ˆä¸“ä¸šåˆä½œè°ƒèŠ‚é¢†å¯¼åŠ›å¯¹å¸ˆç”Ÿå…³ç³»çš„å½±å“ï¼š
#äº¤äº’é¡¹éœ€è¦æå‰åŠ å…¥æ•°æ®é›†ä¹‹ä¸­
# è®¡ç®—äº¤äº’é¡¹ï¼Œå¹¶åŠ å…¥æ•°æ®é›†
summary(merged_data$T3PLEADS_score)
fit4 <- sem(model4, data = merged_data, missing = "fiml")
summary(fit4, fit.measures = TRUE, standardized = TRUE)
summary(fit1, fit.measures = TRUE, standardized = TRUE)
library(car)
install.packages("car")
library(car)
model <- lm(T3COLES ~ TT3G33A + TT3G33B + TT3G33C + TT3G33H + T3PLEADS_score, data = data_frame)
model <- lm(T3COLES ~ TT3G33A + TT3G33B + TT3G33C + TT3G33H + T3PLEADS_score, data = merged_data)
model <- lm(T3COLES_score ~ TT3G33A + TT3G33B + TT3G33C + TT3G33H + T3PLEADS_score, data = merged_data)
vif(model)
vif_values <- vif(model)
tolerance_values <- 1 / vif_values
tolerance_values
model <- lm(T3PLEADS_score ~ TT3G33A + TT3G33B + TT3G33C + TT3G33H , data = merged_data)
vif(model)
vif_values <- vif(model)
tolerance_values <- 1 / vif_values
tolerance_values
model <- lm(T3PLEADS_score ~ TT3G33A + TT3G33B + TT3G33C + TT3G33H+TT3G33A + TT3G33B + TT3G33C + TT3G33H , data = merged_data)
vif(model)
vif_values <- vif(model)
model <- lm(T3PLEADS_score ~ TT3G49A + TT3G49B + TT3G49C + TT3G49D+TT3G33A + TT3G33B + TT3G33C + TT3G33H , data = merged_data)
vif(model)
vif_values <- vif(model)
tolerance_values <- 1 / vif_values
tolerance_values
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹1
fit_group1 <- lavaan::sem(model1, data = merged_data, group = "CNTRY")
summary(fit_group1, fit.measures = TRUE)
# æå–å›å½’ç³»æ•°å’Œå‚æ•°ä¼°è®¡
param_estimates1 <- lavaan::parameterEstimates(fit_group1)
regression_estimates1 <- param_estimates1[param_estimates$op == "~", ]
# æŒ‰ç…§å›½å®¶åˆ†ç»„æŸ¥çœ‹ç»“æœ
split(regression_estimates1, regression_estimates1$group)
merged_data$CNTRY
#########æ ¡é•¿å­¦ä¹ é¢†å¯¼åŠ› â†’ æ•™å¸ˆæƒ…æ„Ÿ
model2 <- '
# æ•™å¸ˆæƒ…æ„Ÿ
T3TEAM =~ TT3G32A + TT3G32B + TT3G32C + TT3G32D
T3SELF =~ TT3G34A + TT3G34B + TT3G34C + TT3G34D + TT3G34E + TT3G34F + TT3G34G + TT3G34H + TT3G34I + TT3G34J + TT3G34K + TT3G34L
TT3G49E ~~ TT3G49E  # è¿™ä¸ªå˜é‡åªæœ‰ä¸€ä¸ªè§‚æµ‹é‡ï¼Œä¸èƒ½å»ºå› å­ï¼Œç›´æ¥å»ºå›å½’
# é¢†å¯¼åŠ›å½±å“æ•™å¸ˆæƒ…æ„Ÿ
T3TEAM ~ T3PLEADS_score
T3SELF ~ T3PLEADS_score
TT3G49E ~ T3PLEADS_score
'
fit2 <- sem(model2, data = merged_data, missing = "fiml")
summary(fit2, fit.measures = TRUE, standardized = TRUE)
#æ ¡é•¿å­¦ä¹ é¢†å¯¼åŠ› â†’ è¯¾å ‚æ•™å­¦
model3 <- '
# è¯¾å ‚æ•™å­¦
T3TPRA =~ TT3G42A + TT3G42B + TT3G42C + TT3G42D + TT3G42E + TT3G42F + TT3G42G + TT3G42H + TT3G42I + TT3G42J + TT3G42K + TT3G42L
T3DISC =~ TT3G41A + TT3G41B + TT3G41C + TT3G41D
# é¢†å¯¼åŠ›å½±å“è¯¾å ‚æ•™å­¦
T3TPRA ~ T3PLEADS_score
T3DISC ~ T3PLEADS_score
'
fit3 <- sem(model3, data = merged_data, missing = "fiml")
summary(fit3, fit.measures = TRUE, standardized = TRUE)
model4 <- '
# ç»„ç»‡æ°›å›´ï¼ˆæ½œå˜é‡ï¼‰
T3STUD =~ TT3G49A + TT3G49B + TT3G49C + TT3G49D
T3COLES =~ TT3G33A + TT3G33B + TT3G33C + TT3G33H
# é¢†å¯¼åŠ›å½±å“å¸ˆç”Ÿå…³ç³»ï¼Œæ•™å¸ˆä¸“ä¸šåˆä½œä¸ºè°ƒèŠ‚å˜é‡
T3STUD ~ T3PLEADS_score + T3COLES + interaction_term
'
fit4 <- sem(model4, data = merged_data, missing = "fiml")
summary(fit4, fit.measures = TRUE, standardized = TRUE)
model4 <- '
# ç»„ç»‡æ°›å›´ï¼ˆæ½œå˜é‡ï¼‰
T3STUD =~ TT3G49A + TT3G49B + TT3G49C + TT3G49D
T3COLES =~ TT3G33A + TT3G33B + TT3G33C + TT3G33H
# é¢†å¯¼åŠ›å½±å“å¸ˆç”Ÿå…³ç³»ï¼Œæ•™å¸ˆä¸“ä¸šåˆä½œä¸ºè°ƒèŠ‚å˜é‡
T3PLEADS_score ~T3STUD  + T3COLES + interaction_term
'
fit4 <- sem(model4, data = merged_data, missing = "fiml")
summary(fit4, fit.measures = TRUE, standardized = TRUE)
merged_data$interaction_term <- merged_data$T3PLEADS_score * merged_data$T3COLES_score
model4 <- '
# ç»„ç»‡æ°›å›´ï¼ˆæ½œå˜é‡ï¼‰
T3STUD =~ TT3G49A + TT3G49B + TT3G49C + TT3G49D
T3COLES =~ TT3G33A + TT3G33B + TT3G33C + TT3G33H
# é¢†å¯¼åŠ›å½±å“å¸ˆç”Ÿå…³ç³»ï¼Œæ•™å¸ˆä¸“ä¸šåˆä½œä¸ºè°ƒèŠ‚å˜é‡
T3PLEADS_score ~T3STUD  + T3COLES + interaction_term
'
fit4 <- sem(model4, data = merged_data, missing = "fiml")
summary(fit4, fit.measures = TRUE, standardized = TRUE)
# å†æ¬¡è¿è¡ŒSEMæ¨¡å‹4
fit_group4 <- lavaan::sem(model4, data = merged_data, group = "CNTRY")
summary(fit_group4, fit.measures = TRUE)
# æå–å›å½’ç³»æ•°å’Œå‚æ•°ä¼°è®¡
param_estimates4 <- lavaan::parameterEstimates(fit_group4)
regression_estimates4 <- param_estimates4[param_estimates$op == "~", ]
# æŒ‰ç…§å›½å®¶åˆ†ç»„æŸ¥çœ‹ç»“æœ
split(regression_estimates4, regression_estimates4$group)
cancer<-read.table(url("http://www.stats.gla.ac.uk/~tereza/rp/cancer.txt"),
header=TRUE)
gc()
cancer<-read.table(url("http://www.stats.gla.ac.uk/~tereza/rp/cancer.txt"),
header=TRUE)
head(cancer)
ggpairs(cancer[,c(-1,-3)],
upper=list(continuous=wrap("points",alpha=0.4, color="#d73027")),
lower="blank",axisLabels="none")
library(ggplot2)
ggpairs(cancer[,c(-1,-3)],
upper=list(continuous=wrap("points",alpha=0.4, color="#d73027")),
lower="blank",axisLabels="none")
library(GGally)
install.packages("GGally")
ggpairs(cancer[,c(-1,-3)],
upper=list(continuous=wrap("points",alpha=0.4, color="#d73027")),
lower="blank",axisLabels="none")
library(GGally)
ggpairs(cancer[,c(-1,-3)],
upper=list(continuous=wrap("points",alpha=0.4, color="#d73027")),
lower="blank",axisLabels="none")
cancer<-read.table(url("http://www.stats.gla.ac.uk/~tereza/rp/cancer.txt"),
header=TRUE)
head(cancer)
library(GGally)
ggpairs(cancer[,c(-1,-3)],
upper=list(continuous=wrap("points",alpha=0.4, color="#d73027")),
lower="blank",axisLabels="none")
epid1<-glm(Y_all~pm10+smoke+ethnic+log.price+easting+
northing+offset(log(E_all)), family=poisson, data=cancer)
summary(epid1)
library(faraway)
install.packages("faraway")
library(faraway)
head(gala)
ggpairs(gala, upper=list(continuous=wrap("points", alpha=0.4, color="#d73027")),
lower="blank", axisLabels="none")
gal1 <- glm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent,
family = poisson, data = gala)
summary(gal1)
# è®¡ç®— Pearson ç»Ÿè®¡é‡
pearson_chi_sq <- sum(residuals(gal1, type = "pearson")^2)
pearson_chi_sq
resp<-resid(gal1,type= "pearson")
resp
resd<-resid(gal1,type= "deviance")
p1<-ggplot(gal1,aes(sample=resp))+geom_point(stat="qq",color= "#7fc97f")+
ylab("Pearsonresiduals")
p2<-ggplot(gal1,aes(sample =resd))+geom_point(stat="qq",color= "#7fc97f")+
ylab("Devianceresiduals")
p3<-ggplot(gal1,aes(x=predict(gal1,type="link"), y=resd))+
geom_point(col="#7fc97f")+
ylab("Devianceresiduals")+xlab("Linearpredictor")
grid.arrange(p1,p2,p3,nrow=1)
library(gridExtra)
grid.arrange(p1,p2,p3,nrow=1)
p4<-ggplot(gal1,aes(x=predict(gal1,type="link"), y=resp))+
geom_point(col="#7fc97f")+
ylab("Devianceresiduals")+xlab("Linearpredictor")
library(gridExtra)
grid.arrange(p1,p2,p3,p4,nrow=2)
ggplot(gal1,aes(x=log(fitted(gal1)), y=log((gala$Species-fitted(gal1))Ë†2)))+
ggplot(gal1,aes(x=log(fitted(gal1)), y=log((gala$Species-fitted(gal1))Ë†2)))+
ggplot(gal1,aes(x=log(fitted(gal1)), y=log((gala$Species-fitted(gal1))Ë†2)))+
ggplot(gal1,aes(x=log(fitted(gal1)), y=log((gala$Species-fitted(gal1))^2)))+
geom_point(col="#f46d43")+
geom_abline(slope=1,intercept=0,col="#a6d96a",size=1)+
ylab(expression((y-hat(mu))^2))+
xlab(expression(hat(mu)))
#######################
X2 <- sum(resid(gal1, type = "pearson")Ë†2)#pearson_chi_sq
#######################
X2 <- sum(resid(gal1, type = "pearson")^2)#pearson_chi_sq
dp <- X2 / gal1$df.res
dp
summary(gal1, dispersion = dp)
summary(gal1)
drop1(gal1, test = "F")
# Residual plots vs. predicted
pred <- predict(gal1, type = "response")
stand.resid <- rstandard(model = gal1, type = "pearson") # Standardised Pearson residuals
par(mfrow=c(1,2))
plot(x = pred, y = stand.resid, xlab = "Predicted count", ylab = "Standardised Pearson residuals",
main = "Regular likelihood", ylim = c(-5,5))
abline(h = c(-3,-2, 0, 2, 3), lty = "dotted", col = "red")
gal2 <- glm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent,
family = quasipoisson(link = "log"), data = gala) # Quasi-Poisson model
pred <- predict(gal2, type = "response")
stand.resid <- rstandard(model = gal2, type = "pearson") # Standardised Pearson residuals
plot(x = pred, y = stand.resid, xlab = "Predicted count", ylab = "Standardised Pearson residuals",
main = "Quasi-likelihood", ylim = c(-5,5))
abline(h = c(-3,-2, 0, 2, 3), lty = "dotted", col = "red")
####################Negativebinomialmodel
library(MASS)
gal3<-glm.nb(Species~Area+Elevation+Nearest+Scruz+Adjacent,
data= gala)
summary(gal3)
c(gal1$deviance, gal1$aic)
c(gal3$deviance, gal3$aic)
# Plot of squared residuals v predicted values
res.sq <- residuals(gal1, type = "response")Ë†2
# Plot of squared residuals v predicted values
res.sq <- residuals(gal1, type = "response")^2
set1 <- data.frame(res.sq, mu.hat = gal1$fitted.values)
fit.lin <- lm(formula = res.sq ~ mu.hat, data = set1)
fit.quad <- lm(formula = res.sq ~ mu.hat + I(mu.hat^2), data = set1)
summary(fit.quad)
plot(set1$mu.hat,y= set1$res.sq, xlab="Predictedcount",
ylab="SquaredResidual")
curve(expr= predict(fit.lin,newdata= data.frame(mu.hat=x),type= "response"),
col="blue", add=TRUE, lty="solid")
curve(expr= predict(fit.quad,newdata= data.frame(mu.hat=x), type="response"),
col="red", add=TRUE,lty="dashed")
legend("topleft",legend= c("Linear","Quadratic"),col= c("blue","red"),
lty=c("solid","dashed"), bty="n")
par(mfrow=c(1,1))
plot(set1$mu.hat,y= set1$res.sq, xlab="Predictedcount",
ylab="SquaredResidual")
curve(expr= predict(fit.lin,newdata= data.frame(mu.hat=x),type= "response"),
col="blue", add=TRUE, lty="solid")
curve(expr= predict(fit.quad,newdata= data.frame(mu.hat=x), type="response"),
col="red", add=TRUE,lty="dashed")
legend("topleft",legend= c("Linear","Quadratic"),col= c("blue","red"),
lty=c("solid","dashed"), bty="n")
###############
olympics0<-read.csv(url("http://www.stats.gla.ac.uk/~tereza/rp/OlympicMedals2012.csv"))
olympics<-data.frame(country= olympics0$Country, medals=olympics0$Medals,
population=olympics0$Population,
gold =olympics0$Gold.Medal,
GDP=olympics0$GDP..US.Billion)
olympics$GDPpercapita<-olympics$GDP*10Ë†6/olympics$population
head(olympics)
olympics$GDPpercapita<-olympics$GDP*10^6/olympics$population
head(olympics)
p1<-ggplot(olympics,aes(x=log(population), y=log(medals)))+
geom_point(col="#f46d43")
p2<-ggplot(olympics,aes(x=log(GDPpercapita), y=log(medals)))+
geom_point(col="#f46d43")
grid.arrange(p1,p2,nrow=1)
ol1<-glm(medals~log(population)+log(GDPpercapita),
family=poisson,data= olympics)
summary(ol1)
setwd("D:/work/Github/Data-Mining")
library(dplyr)
data<-read.csv("group_37.csv")
total_na <- sum(is.na(data))
total_na
sum(is.na(data))
is.na(data)
sum(is.na(data))
View(data)
target <- data[, 1]
features <- data[, -1]
feature_variances <- apply(features, 2, var, na.rm = TRUE)
feature_variances <- apply(features, 2, var)
boxplot(feature_variances, col = "lightblue", border = "blue",horizontal = FALSE)
boxplot(feature_variances, col = "lightblue", border = "blue"ï¼‰
boxplot(feature_variances, col = "lightblue", border = "blue")
variance_threshold <- 0.01
selected_features_var <- features[, feature_variances > variance_threshold]
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œå‰©ä½™ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
selected_features_var <- features[, feature_variances > variance_threshold]
removed_features_var <- features[, feature_variances < variance_threshold]
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œå‰©ä½™ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œç§»é™¤ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
#person
cor_values <- apply(selected_features_var, 2, function(x) cor(x, target, method = "spearman", use = "complete.obs"))
cor_threshold <- 0.2
selected_features_cor <- selected_features_var[, abs(cor_values) > cor_threshold]
cat("ğŸ”¹ ç»è¿‡ç›¸å…³æ€§ç­›é€‰ï¼Œæœ€ç»ˆå‰©ä½™ç‰¹å¾æ•°:", ncol(selected_features_cor), "\n")
variance_threshold <- 0.001
selected_features_var <- features[, feature_variances > variance_threshold]
removed_features_var <- features[, feature_variances < variance_threshold]
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œå‰©ä½™ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œç§»é™¤ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œç§»é™¤ç‰¹å¾æ•°:", ncol(removed_features_var), "\n")
variance_threshold <- 0.01
selected_features_var <- features[, feature_variances > variance_threshold]
removed_features_var <- features[, feature_variances < variance_threshold]
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œå‰©ä½™ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œç§»é™¤ç‰¹å¾æ•°:", ncol(removed_features_var), "\n")
variance_threshold <- 0.02
selected_features_var <- features[, feature_variances > variance_threshold]
removed_features_var <- features[, feature_variances < variance_threshold]
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œå‰©ä½™ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œç§»é™¤ç‰¹å¾æ•°:", ncol(removed_features_var), "\n")
variance_threshold <- 0.1
selected_features_var <- features[, feature_variances > variance_threshold]
removed_features_var <- features[, feature_variances < variance_threshold]
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œå‰©ä½™ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œç§»é™¤ç‰¹å¾æ•°:", ncol(removed_features_var), "\n")
variance_threshold <- 0.2
selected_features_var <- features[, feature_variances > variance_threshold]
removed_features_var <- features[, feature_variances < variance_threshold]
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œå‰©ä½™ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œç§»é™¤ç‰¹å¾æ•°:", ncol(removed_features_var), "\n")
variance_threshold <- 0.4
selected_features_var <- features[, feature_variances > variance_threshold]
removed_features_var <- features[, feature_variances < variance_threshold]
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œå‰©ä½™ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œç§»é™¤ç‰¹å¾æ•°:", ncol(removed_features_var), "\n")
#æ–¹å·®
feature_variances <- apply(features, 2, sd)
boxplot(feature_variances, col = "lightblue", border = "blue")
variance_threshold <- 0.01
selected_features_var <- features[, feature_variances > variance_threshold]
removed_features_var <- features[, feature_variances < variance_threshold]
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œå‰©ä½™ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œç§»é™¤ç‰¹å¾æ•°:", ncol(removed_features_var), "\n")
variance_threshold <- 0.1
selected_features_var <- features[, feature_variances > variance_threshold]
removed_features_var <- features[, feature_variances < variance_threshold]
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œå‰©ä½™ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œç§»é™¤ç‰¹å¾æ•°:", ncol(removed_features_var), "\n")
variance_threshold <- 200
selected_features_var <- features[, feature_variances > variance_threshold]
removed_features_var <- features[, feature_variances < variance_threshold]
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œå‰©ä½™ç‰¹å¾æ•°:", ncol(selected_features_var), "\n")
cat("ğŸ”¹ ç»è¿‡æ–¹å·®ç­›é€‰ï¼Œç§»é™¤ç‰¹å¾æ•°:", ncol(removed_features_var), "\n")
filtered_data <- cbind(target, selected_features_var)
target <- filtered_data[, 1]  # ç›®æ ‡å˜é‡
features <- filtered_data[, -1]  # ç‰¹å¾æ•°æ®
# ===== 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† =====
set.seed(123)  # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°
trainIndex <- createDataPartition(target, p = 0.7, list = FALSE)  # 70%è®­ç»ƒé›†ï¼Œ30%æµ‹è¯•é›†
trainData <- filtered_data[trainIndex, ]  # è®­ç»ƒæ•°æ®
library(randomForest)
library(caret)
install.packages("randomForest")
library(randomForest)
library(caret)
target <- filtered_data[, 1]  # ç›®æ ‡å˜é‡
features <- filtered_data[, -1]  # ç‰¹å¾æ•°æ®
# ===== 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† =====
set.seed(123)  # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°
trainIndex <- createDataPartition(target, p = 0.7, list = FALSE)  # 70%è®­ç»ƒé›†ï¼Œ30%æµ‹è¯•é›†
trainData <- filtered_data[trainIndex, ]  # è®­ç»ƒæ•°æ®
testData <- filtered_data[-trainIndex, ]  # æµ‹è¯•æ•°æ®
train_target <- target[trainIndex]  # è®­ç»ƒé›†ç›®æ ‡å˜é‡
test_target <- target[-trainIndex]  # æµ‹è¯•é›†ç›®æ ‡å˜é‡
train_features <- features[trainIndex, ]  # è®­ç»ƒé›†ç‰¹å¾
test_features <- features[-trainIndex, ]  # æµ‹è¯•é›†ç‰¹å¾
# ===== 3. ä½¿ç”¨é€»è¾‘å›å½’è¿›è¡Œè®­ç»ƒ =====
# æ„å»ºé€»è¾‘å›å½’æ¨¡å‹
log_model <- glm(train_target ~ ., data = trainData, family = binomial)
# ===== 3. ä½¿ç”¨é€»è¾‘å›å½’è¿›è¡Œè®­ç»ƒ =====
# æ„å»ºé€»è¾‘å›å½’æ¨¡å‹
train_target <- ifelse(train_target == -1, 0, train_target)
test_target <- ifelse(test_target == -1, 0, test_target)
log_model <- glm(train_target ~ ., data = trainData, family = binomial)
# ===== 4. å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ =====
# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
log_predictions <- predict(log_model, testData, type = "response")
library(dplyr)
# ===== 4. å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ =====
# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
log_predictions <- predict(log_model, testData, type = "response")
log_predictions <- ifelse(log_predictions > 0.5, 1, -1)  # å°†æ¦‚ç‡è½¬åŒ–ä¸ºç±»åˆ«ï¼ˆ1 æˆ– -1ï¼‰
# ===== 5. æ¨¡å‹è¯„ä¼° =====
# æ··æ·†çŸ©é˜µ
confusionMatrix(as.factor(log_predictions), as.factor(test_target))
# ===== 6. ä½¿ç”¨éšæœºæ£®æ—è¿›è¡Œè®­ç»ƒï¼ˆå¯é€‰ï¼‰ =====
rf_model <- randomForest(train_target ~ ., data = trainData)
# å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
rf_predictions <- predict(rf_model, testData)
# éšæœºæ£®æ—çš„æ··æ·†çŸ©é˜µ
confusionMatrix(as.factor(rf_predictions), as.factor(test_target))
log_model <- glm(train_target ~ ., data = trainData, family = binomial)
log_model$deviance
log_model$null.deviance
# ===== 4. å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ =====
# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
log_predictions <- predict(log_model, testData, type = "response")
pca <- prcomp(train_features, center = TRUE, scale. = TRUE)
summary(pca)
train_pca_features <- pca$x[, 1:10]
test_pca_features <- predict(pca, newdata = test_features)[, 1:10]
log_model <- glm(train_target ~ ., data = data.frame(train_pca_features), family = binomial)
# ===== 6. å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ =====
log_predictions <- predict(log_model, data.frame(test_pca_features), type = "response")
log_predictions <- ifelse(log_predictions > 0.5, 1, 0)  # å°†æ¦‚ç‡è½¬åŒ–ä¸ºç±»åˆ«ï¼ˆ0 æˆ– 1ï¼‰
# ===== 7. æ¨¡å‹è¯„ä¼° =====
confusionMatrix(as.factor(log_predictions), as.factor(test_target))
# ===== 8. ä½¿ç”¨éšæœºæ£®æ—è¿›è¡Œè®­ç»ƒï¼ˆå¯é€‰ï¼‰ =====
rf_model <- randomForest(train_target ~ ., data = data.frame(train_pca_features))
# å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
rf_predictions <- predict(rf_model, data.frame(test_pca_features))
# éšæœºæ£®æ—çš„æ··æ·†çŸ©é˜µ
confusionMatrix(as.factor(rf_predictions), as.factor(test_target))
# ===== 7. æ¨¡å‹è¯„ä¼° =====
confusionMatrix(as.factor(log_predictions), as.factor(test_target))
