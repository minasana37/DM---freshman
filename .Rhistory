library(ggplot2)
pred.prob <- predict(Model.bagging,newdata = data.test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data.test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
bagg.pred <- predict(Model.bagging,newdata = data.test[,-1],type="class")
test.table.bagg <- table(data.test[,1],bagg.pred)
print(test.table.bagg)
accuracy <- sum(diag(test.table.bagg))/sum(test.table.bagg)
print(accuracy)
train.pca$rotation
loading.matrix <- train.pca$rotation #matrix
#获取方差贡献率
variable_contribution <- rowSums(train.pca$rotation[,1:99]^2)
cumulate_variance <- cumsum(sort(variable_contribution,decreasing = T))
threshold <- 0.95*sum(variable_contribution)
#选出贡献值95%的
selected_variance <- which(cumulate_variance <= threshold)
filtered_data <- data.train[,c(1,selected_variance+1)]
#classification
library(rpart)
library(rpart.plot)
set.seed(2023)
model.classification <- rpart(Class~.,data=filtered_data,method="class",cp=-1,minsplit=2,minbucket=1)
printcp(model.classification)
plotcp(model.classification)
model.classification <- prune(model.classification,cp=0.13)
model.classification
rpart.plot(model.classification,type = 2,extra = 4)
model.classification$variable.importance
barplot(model.classification$variable.importance,col="lightpink",main = "variable Importance")
library(ROCR)
library(ggplot2)
pred.prob <- predict(model.classification,newdata = data.test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data.test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
class.pred <- predict(model.classification,newdata = data.test[,-1],type="class")
test.table.class <- table(data.test[,1],class.pred)
print(test.table.class)
accuracy <- sum(diag(test.table.class))/sum(test.table.class)
print(accuracy)
library(randomForest)
#bagging
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=filtered_data,mtry=ncol(data.train)-1,ntree=500)
Model.bagging
plot(Model.bagging$err.rate[,1],type = "l",
xlab = "Number of trees",ylab="OOB Error",main="OBB Error vs. Number of Trees")
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=filtered_data,mtry=ncol(data.train)-1,ntree=300)
Model.bagging
importance(Model.bagging)
varImpPlot(Model.bagging, main="predicting class")
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.bagging,newdata = data.test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data.test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
bagg.pred <- predict(Model.bagging,newdata = data.test[,-1],type="class")
test.table.bagg <- table(data.test[,1],bagg.pred)
print(test.table.bagg)
accuracy <- sum(diag(test.table.bagg))/sum(test.table.bagg)
print(accuracy)
#random forest
set.seed(2023)
Model.random <- randomForest(Class~.,data=filtered_data,ntree=500)
Model.random
varImpPlot(Model.random, main="predicting class")
random.pred <- predict(Model.random,newdata = data.test[,-1],type="class")
test.table.random <- table(data.test[,1],random.pred)
test.table.random[1,1]/sum(test.table.random[1,])
test.table.random[2,2]/sum(test.table.random[2,])
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.random,newdata = data.test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data.test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
View(filtered_data)
View(loading.matrix)
View(filtered_data)
#random forest
set.seed(2023)
Model.random <- randomForest(Class~.,data=data.train,ntree=500)
Model.random
plot(Model.random$err.rate[,1],type = "l",
xlab = "Number of trees",ylab="OOB Error",main="OBB Error vs. Number of Trees")
plot(Model.random$err.rate[,1],type = "l",
xlab = "Number of trees",ylab="OOB Error",main="OBB Error vs. Number of Trees")
varImpPlot(Model.random, main="predicting class")
random.pred <- predict(Model.random,newdata = data.test[,-1],type="class")
test.table.random <- table(data.test[,1],random.pred)
test.table.random[1,1]/sum(test.table.random[1,])
test.table.random[2,2]/sum(test.table.random[2,])
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.random,newdata = data.test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data.test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
random.pred <- predict(Model.random,newdata = data.test[,-1],type="class")
test.table.random <- table(data.test[,1],random.pred)
print(test.table.random)
accuracy <- sum(diag(test.table.random))/sum(test.table.random)
print(accuracy)
train.pca$x
view(titanic)
View(titanic)
library(titanic)
library(titanic)
install.packages("titanic")
View(Titanic)
#random forest
set.seed(2023)
Model.random <- randomForest(Class~.,data=data.train[,-1],ntree=500)
#random forest
set.seed(2023)
Model.random <- randomForest(Class~.,data=data.train,ntree=500)
Model.random
set.seed(2023)
Model.random <- randomForest(Class~.,data=data.train,ntree=200)
Model.random
varImpPlot(Model.random, main="predicting class")
#random forest
set.seed(2023)
Model.random <- randomForest(Class~.,data=data.train,ntree=500)
Model.random
varImpPlot(Model.random, main="predicting class")
importance(Model.random)
varImpPlot(Model.random, main="predicting class")
#use PCA
pca<-prcomp(selected_features)
#packages
library(dplyr)
library(neuralnet)
data_train <- read.csv("group_37.csv")
data_test <- read.csv("arcene_test (for groups 32-38).csv")
sum(is.na(data_train))
sum(is.na(data_test))
ncol(data_train) ==
sum(colnames(data_train) %in% colnames(data_test))
class <- data_train[, 1]
features <- data_train[, -1]
data_train[, 1] <- ifelse(data_train[, 1] == -1, 0, 1)
data_train <- read.csv("group_37.csv")
data_test <- read.csv("arcene_test (for groups 32-38).csv")
sum(is.na(data_train))
sum(is.na(data_test))
ncol(data_train) ==
sum(colnames(data_train) %in% colnames(data_test))
class <- data_train[, 1]
features <- data_train[, -1]
data_train[, 1] <- ifelse(data_train[, 1] == -1, 0, 1)
View(data_train)
feature_sd <- apply(features, 2, sd)
boxplot(feature_sd)
q1 <- quantile(feature_sd, 0.25)
q3 <- quantile(feature_sd, 0.75)
sd_threshold <- q3-q1
selected_features <- features[, feature_sd > sd_threshold]
removed_features <- features[, feature_sd <= sd_threshold]
cat("remaing features", ncol(selected_features), "\n")
cat("removed features:", ncol(removed_features), "\n")
#use PCA
pca<-prcomp(selected_features)
sd.pca <- pca$sdev
#based on kaiser
tot.var<-sum(sd.pca^2)
ave.var<-tot.var/ncol(selected_features)
ave.var
sd.pca^2 > ave.var
all_variance <- sd.pca^2
cum_variance <- cumsum(all_variance) / sum(all_variance)
selected_components <- which(sd.pca^2 > ave.var)
selected_variance <- sum(all_variance[selected_components]) / sum(all_variance)
cat("Cumulative contribution:", selected_variance, "\n")
pca_features1 <- pca$x[, 1:47]
#based on Cumulative variance contribution 95%
cumulative_variance <- cumsum(sd.pca^2) / sum(sd.pca^2)
selected_components_cumulative <- which(cumulative_variance >= 0.95)[1]
selected_components_cumulative
pca_features2 <- pca$x[, 1:selected_components_cumulative]
traindata <- data.frame(pca_features1, Class = data_train[[1]])
data_train$Class<- as.factor(data_train$Class)
str(traindata)
View(traindata)
#classification
library(rpart)
library(rpart.plot)
set.seed(2023)
model.classification <- rpart(Class~.,data=traindata,method="class",cp=-1,minsplit=2,minbucket=1)
printcp(model.classification)
plotcp(model.classification)
model.classification <- prune(model.classification,cp=0.032)
model.classification
rpart.plot(model.classification,type = 2,extra = 4)
model.classification$variable.importance
barplot(model.classification$variable.importance,col="lightpink",main = "variable Importance")
library(ROCR)
library(ggplot2)
pred.prob <- predict(model.classification,newdata = data.test,type = "prob")[,2]
library(ROCR)
library(ggplot2)
pred.prob <- predict(model.classification,newdata = data_test,type = "prob")[,2]
{r}
library(ROCR)
library(ggplot2)
pred.prob <- predict(model.classification,newdata = testdata,type = "prob")[,2]
data_test[, 1] <- ifelse(data_test[, 1] == -1, 0, 1)
selected_features_names <- colnames(selected_features)
data_test_new <- data_test[, selected_features_names]
testdata <- data.frame(
predict(pca, newdata = data_test_new)[,1:47],
Class = data_test[[1]])
str(testdata)
library(ROCR)
library(ggplot2)
pred.prob <- predict(model.classification,newdata = testdata,type = "prob")[,2]
true_labels <- as.numeric(as.character(data.test$Class))
library(ROCR)
library(ggplot2)
pred.prob <- predict(model.classification,newdata = testdata,type = "prob")[,2]
true_labels <- as.numeric(as.character(testdata$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
class.pred <- predict(model.classification,newdata = data_test[,-1],type="class")
View(testdata)
View(data_test_new)
View(testdata)
class.pred <- predict(model.classification,newdata = testdata,type="class")
test.table.class <- table(testdata$Class,class.pred)
print(test.table.class)
accuracy <- sum(diag(test.table.class))/sum(test.table.class)
print(accuracy)
library(randomForest)
#bagging
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=data_train,mtry=ncol(data_train)-1,ntree=500)
Model.bagging
plot(Model.bagging$err.rate[,1],type = "l",
xlab = "Number of trees",ylab="OOB Error",main="OBB Error vs. Number of Trees")
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=filtered_data,mtry=ncol(data.train)-1,ntree=300)
importance(Model.bagging)
varImpPlot(Model.bagging, main="predicting class")
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.bagging,newdata = data.test,type = "prob")[,2]
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.bagging,newdata = data_test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data_test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
bagg.pred <- predict(Model.bagging,newdata = data_test[,-1],type="class")
test.table.bagg <- table(data.test[,1],bagg.pred)
bagg.pred <- predict(Model.bagging,newdata = data_test[,-1],type="class")
test.table.bagg <- table(data_test[,1],bagg.pred)
print(test.table.bagg)
accuracy <- sum(diag(test.table.bagg))/sum(test.table.bagg)
print(accuracy)
#random forest
set.seed(2023)
Model.random <- randomForest(Class~.,data=data_train,ntree=500)
Model.random
plot(Model.random$err.rate[,1],type = "l",
xlab = "Number of trees",ylab="OOB Error",main="OBB Error vs. Number of Trees")
importance(Model.random)
varImpPlot(Model.random, main="predicting class")
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.random,newdata = data_test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data_test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=data_train,mtry=ncol(data.train)-1,ntree=200)
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=data_train,mtry=ncol(data_train)-1,ntree=200)
Model.bagging
importance(Model.bagging)
varImpPlot(Model.bagging, main="predicting class")
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.bagging,newdata = data_test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data_test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
bagg.pred <- predict(Model.bagging,newdata = data_test[,-1],type="class")
test.table.bagg <- table(data_test[,1],bagg.pred)
print(test.table.bagg)
accuracy <- sum(diag(test.table.bagg))/sum(test.table.bagg)
print(accuracy)
random.pred <- predict(Model.random,newdata = data.test[,-1],type="class")
random.pred <- predict(Model.random,newdata = data_test[,-1],type="class")
test.table.random <- table(data_test[,1],random.pred)
print(test.table.random)
accuracy <- sum(diag(test.table.random))/sum(test.table.random)
print(accuracy)
View(data_train)
View(selected_features)
0.54545+ 0.097064
traindata2 <- data.frame(selected_features,Class = data_train[1])
testdata2 <- data.frame(data_test_new,Class = data_test[1])
traindata2 <- data.frame(selected_features,Class = data_train[1])
testdata2 <- data.frame(data_test_new,Class = data_test[1])
library(randomForest)
#bagging
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=traindata2,mtry=ncol(traindata2)-1,ntree=500)
Model.bagging
plot(Model.bagging$err.rate[,1],type = "l",
xlab = "Number of trees",ylab="OOB Error",main="OBB Error vs. Number of Trees")
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=data_train,mtry=ncol(data_train)-1,ntree=200)
Model.bagging
importance(Model.bagging)
varImpPlot(Model.bagging, main="predicting class")
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.bagging,newdata = testdata2,type = "prob")[,2]
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.bagging,newdata = testdata2,type = "prob")[,2]
bagg.pred <- predict(Model.bagging,newdata = testdata2[,-1],type="class")
View(testdata2)
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=traindata2,mtry=ncol(traindata2)-1,ntree=200)
Model.bagging
importance(Model.bagging)
varImpPlot(Model.bagging, main="predicting class")
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.bagging,newdata = testdata2,type = "prob")[,2]
true_labels <- as.numeric(as.character(testdata2$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
bagg.pred <- predict(Model.bagging,newdata = testdata2[,-1],type="class")
bagg.pred <- predict(Model.bagging,newdata = testdata2[,-ncol(testdata2)],type="class")
test.table.bagg <- table(testdata2$Class,bagg.pred)
print(test.table.bagg)
accuracy <- sum(diag(test.table.bagg))/sum(test.table.bagg)
print(accuracy)
#random forest
set.seed(2023)
Model.random <- randomForest(Class~.,data=traindata2,ntree=500)
Model.random
plot(Model.random$err.rate[,1],type = "l",
xlab = "Number of trees",ylab="OOB Error",main="OBB Error vs. Number of Trees")
importance(Model.random)
varImpPlot(Model.random, main="predicting class")
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.random,newdata = data_test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data_test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
random.pred <- predict(Model.random,newdata = data_test[,-1],type="class")
test.table.random <- table(data_test[,1],random.pred)
print(test.table.random)
accuracy <- sum(diag(test.table.random))/sum(test.table.random)
print(accuracy)
class.pred <- predict(model.classification,newdata = testdata,type="class")
test.table.class <- table(testdata$Class,class.pred)
print(test.table.class)
accuracy <- sum(diag(test.table.class))/sum(test.table.class)
print(accuracy)
library(randomForest)
#bagging
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=traindata2,mtry=ncol(traindata2)-1,ntree=500)
Model.bagging
plot(Model.bagging$err.rate[,1],type = "l",
xlab = "Number of trees",ylab="OOB Error",main="OBB Error vs. Number of Trees")
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=traindata2,mtry=ncol(traindata2)-1,ntree=200)
Model.bagging
importance(Model.bagging)
varImpPlot(Model.bagging, main="predicting class")
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.bagging,newdata = testdata2,type = "prob")[,2]
true_labels <- as.numeric(as.character(testdata2$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
bagg.pred <- predict(Model.bagging,newdata = testdata2[,-ncol(testdata2)],type="class")
test.table.bagg <- table(testdata2$Class,bagg.pred)
print(test.table.bagg)
accuracy <- sum(diag(test.table.bagg))/sum(test.table.bagg)
print(accuracy)
#random forest
set.seed(2023)
Model.random <- randomForest(Class~.,data=traindata2,ntree=500)
Model.random
plot(Model.random$err.rate[,1],type = "l",
xlab = "Number of trees",ylab="OOB Error",main="OBB Error vs. Number of Trees")
set.seed(2023)
Model.random <- randomForest(Class~.,data=traindata2,ntree=400)
Model.random
importance(Model.random)
varImpPlot(Model.random, main="predicting class")
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.random,newdata = data_test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data_test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
#random forest
set.seed(2023)
Model.random <- randomForest(Class~.,data=traindata2,ntree=500)
Model.random
plot(Model.random$err.rate[,1],type = "l",
xlab = "Number of trees",ylab="OOB Error",main="OBB Error vs. Number of Trees")
importance(Model.random)
varImpPlot(Model.random, main="predicting class")
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.random,newdata = data_test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data_test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") + ylab("True positive rate") +
ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
random.pred <- predict(Model.random,newdata = data_test[,-1],type="class")
test.table.random <- table(data_test[,1],random.pred)
print(test.table.random)
accuracy <- sum(diag(test.table.random))/sum(test.table.random)
print(accuracy)
