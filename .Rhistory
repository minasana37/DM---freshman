q3 <- quantile(feature_sd, 0.75)
sd_threshold <- q3-q1
selected_features <- features[, feature_sd > sd_threshold]
removed_features <- features[, feature_sd <= sd_threshold]
cat("remaing features", ncol(selected_features), "\n")
cat("removed features:", ncol(removed_features), "\n")
#use PCA
pca<-prcomp(selected_features)
sd.pca <- pca$sdev
#based on kaiser
tot.var<-sum(sd.pca^2)
ave.var<-tot.var/ncol(selected_features)
ave.var
sd.pca^2 > ave.var
all_variance <- sd.pca^2
cum_variance <- cumsum(all_variance) / sum(all_variance)
selected_components <- which(sd.pca^2 > ave.var)
selected_variance <- sum(all_variance[selected_components]) / sum(all_variance)
cat("Cumulative contribution:", selected_variance, "\n")
pca_features1 <- pca$x[, 1:47]
#based on Cumulative variance contribution 95%
cumulative_variance <- cumsum(sd.pca^2) / sum(sd.pca^2)
selected_components_cumulative <- which(cumulative_variance >= 0.95)[1]
selected_components_cumulative
pca_features2 <- pca$x[, 1:selected_components_cumulative]
traindata <- data.frame(pca_features1, Class = data_train[[1]])
traindata2 <- data.frame(pca_features2, Class = data_train[[1]])
traindata3 <- data.frame(selected_features,Class = data_train[1])
data_train$Class<- as.factor(data_train$Class)
str(traindata)
set.seed(2023)
model.classification <- rpart(Class~.,
data=traindata,
method="class",
cp=-1,
minsplit=2,
minbucket=1)
printcp(model.classification)
plotcp(model.classification)
model.classification1 <- prune(model.classification,cp=0.032)
rpart.plot(model.classification1,type = 2,extra = 4)
model.classification1$variable.importance
barplot(model.classification1$variable.importance,
col="lightpink",main = "variable Importance")
#test data process
data_test[, 1] <- ifelse(data_test[, 1] == -1, 0, 1)
selected_features_names <- colnames(selected_features)
data_test_new <- data_test[, selected_features_names]
testdata <- data.frame(
predict(pca, newdata = data_test_new)[,1:47],
Class = data_test[[1]])
testdata2 <- data.frame(
predict(pca, newdata = data_test_new)[,1:18],
Class = data_test[[1]])
testdata3 <- data.frame(data_test_new,Class = data_test[1])
str(testdata)
y_pred_pro <- predict(model.classification1,
newdata = testdata,
type = "prob")[,2]
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))
#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
print(conf_matrix)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
#ROC plot
perfd <- data.frame(x=perf@x.values[1][[1]],
y=perf@y.values[1][[1]])
ggplot(perfd,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") +
ylab("True positive rate") +
ggtitle(paste("Area under the curve:",
round(auc@y.values[[1]], 3)))
#bagging
set.seed(2023)
bagging_Model <- randomForest(Class~.,
data=traindata3,
mtry=ncol(traindata2)-1,
ntree=1000)
bagging_Model
plot(bagging_Model$err.rate[,1],
type = "l",
xlab = "Number of trees",
ylab="OOB Error",
main="OBB Error vs. Number of Trees")
set.seed(2023)
bagging_Model <- randomForest(Class~.,data=traindata3,
mtry=ncol(traindata2)-1,
ntree=200)
bagging_Model
importance(bagging_Model)
varImpPlot(bagging_Model, main="predicting class")
plot(bagging_Model$err.rate[,1],
type = "l",
xlab = "Number of trees",
ylab="OOB Error",
main="OBB Error vs. Number of Trees")
y_pred_pro <- predict(bagging_Model,
newdata = testdata3,
type = "prob")[,2]
set.seed(2023)
bagging_Model <- randomForest(Class~.,data=traindata3,
mtry=ncol(traindata2)-1,
ntree=200,type = "prob")
bagging_Model
#bagging
set.seed(2023)
bagging_Model <- randomForest(Class~.,
data=traindata3,
mtry=ncol(traindata2)-1,
ntree=1000,type = "prob")
bagging_Model
plot(bagging_Model$err.rate[,1],
type = "l",
xlab = "Number of trees",
ylab="OOB Error",
main="OBB Error vs. Number of Trees")
set.seed(2023)
bagging_Model <- randomForest(Class~.,data=traindata3,
mtry=ncol(traindata2)-1,
ntree=200,type = "prob")
bagging_Model
importance(bagging_Model)
varImpPlot(bagging_Model, main="predicting class")
y_pred_pro <- predict(bagging_Model,
newdata = testdata3,
type = "prob")[,2]
y_pred_pro <- predict(bagging_Model,
newdata = testdata3,
type = "prob")
set.seed(2023)
bagging_Model <- randomForest(Class~.,data=traindata3,
mtry=ncol(traindata2)-1,
ntree=200,importance = TRUE)
bagging_Model
y_pred_pro <- predict(bagging_Model,
newdata = testdata3,
type = "prob")[,2]
#bagging
set.seed(2023)
traindata3$Class <- as.factor(traindata3$Class)
bagging_Model <- randomForest(Class~.,
data=traindata3,
mtry=ncol(traindata2)-1,
ntree=1000)
bagging_Model
plot(bagging_Model$err.rate[,1],
type = "l",
xlab = "Number of trees",
ylab="OOB Error",
main="OBB Error vs. Number of Trees")
set.seed(2023)
bagging_Model <- randomForest(Class~.,data=traindata3,
mtry=ncol(traindata2)-1,
ntree=200)
bagging_Model
importance(bagging_Model)
varImpPlot(bagging_Model, main="predicting class")
y_pred_pro <- predict(bagging_Model,
newdata = testdata3,
type = "prob")[,2]
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))
#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
print(conf_matrix)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
#ROC plot
perfd <- data.frame(x=perf@x.values[1][[1]],
y=perf@y.values[1][[1]])
ggplot(perfd,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") +
ylab("True positive rate") +
ggtitle(paste("Area under the curve:",
round(auc@y.values[[1]], 3)))
#random forest
set.seed(2023)
random_Model <- randomForest(Class~.,data=traindata3,ntree=500)
random_Model
plot(Model.random$err.rate[,1],
type = "l",
xlab = "Number of trees",
ylab="OOB Error",
main="OBB Error vs. Number of Trees")
plot(random_Model$err.rate[,1],
type = "l",
xlab = "Number of trees",
ylab="OOB Error",
main="OBB Error vs. Number of Trees")
importance(random_Model)
varImpPlot(random_Model, main="predicting class")
y_pred_pro <- predict(random_Model,
newdata = testdata3,
type = "response")
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))
#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
print(conf_matrix)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
y_pred_pro <- predict(random_Model,
newdata = testdata3,
type = "response")[,2]
y_pred_pro
y_pred_pro <- as.vector(y_pred_pro)
y_pred_pro
y_pred_pro <- predict(random_Model,
newdata = testdata3,
type = "response")[,2]
y_pred_pro
y_pred_pro <- as.numeric(as.vector(y_pred_pro))
y_pred_pro <- predict(random_Model,
newdata = testdata3,
type = "response")[,2]
y_pred_pro <- predict(random_Model,
newdata = testdata3,
type = "prob")[, 2]
y_pred_pro
y_pred_pro <- predict(random_Model,
newdata = testdata3,
type = "prob")[, 2]
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.numeric(as.vector(y_pred_pro))
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))
#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
print(conf_matrix)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
#ROC plot
perfd <- data.frame(x=perf@x.values[1][[1]],
y=perf@y.values[1][[1]])
ggplot(perfd,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") +
ylab("True positive rate") +
ggtitle(paste("Area under the curve:",
round(auc@y.values[[1]], 3)))
# Chunk 1
#packages
library(dplyr)
library(neuralnet)
library(NeuralNetTools)
library(pROC)
library(rpart)
library(rpart.plot)
library(ROCR)
library(ggplot2)
library(randomForest)
# Chunk 2
data_train <- read.csv("group_37.csv")
data_test <- read.csv("arcene_test (for groups 32-38).csv")
sum(is.na(data_train))
sum(is.na(data_test))
ncol(data_train) ==
sum(colnames(data_train) %in% colnames(data_test))
class <- data_train[, 1]
features <- data_train[, -1]
data_train[, 1] <- ifelse(data_train[, 1] == -1, 0, 1)
# Chunk 3
feature_sd <- apply(features, 2, sd)
boxplot(feature_sd)
q1 <- quantile(feature_sd, 0.25)
q3 <- quantile(feature_sd, 0.75)
sd_threshold <- q3-q1
selected_features <- features[, feature_sd > sd_threshold]
removed_features <- features[, feature_sd <= sd_threshold]
cat("remaing features", ncol(selected_features), "\n")
cat("removed features:", ncol(removed_features), "\n")
# Chunk 4
#use PCA
pca<-prcomp(selected_features)
sd.pca <- pca$sdev
#based on kaiser
tot.var<-sum(sd.pca^2)
ave.var<-tot.var/ncol(selected_features)
ave.var
sd.pca^2 > ave.var
all_variance <- sd.pca^2
cum_variance <- cumsum(all_variance) / sum(all_variance)
selected_components <- which(sd.pca^2 > ave.var)
selected_variance <- sum(all_variance[selected_components]) / sum(all_variance)
cat("Cumulative contribution:", selected_variance, "\n")
pca_features1 <- pca$x[, 1:47]
# Chunk 5
#based on Cumulative variance contribution 95%
cumulative_variance <- cumsum(sd.pca^2) / sum(sd.pca^2)
selected_components_cumulative <- which(cumulative_variance >= 0.95)[1]
selected_components_cumulative
pca_features2 <- pca$x[, 1:selected_components_cumulative]
# Chunk 6
traindata <- data.frame(pca_features1, Class = data_train[[1]])
traindata2 <- data.frame(pca_features2, Class = data_train[[1]])
traindata3 <- data.frame(selected_features,Class = data_train[1])
data_train$Class<- as.factor(data_train$Class)
str(traindata)
# Chunk 7
set.seed(2023)
model.classification <- rpart(Class~.,
data=traindata,
method="class",
cp=-1,
minsplit=2,
minbucket=1)
printcp(model.classification)
plotcp(model.classification)
# Chunk 8
model.classification1 <- prune(model.classification,cp=0.032)
rpart.plot(model.classification1,type = 2,extra = 4)
# Chunk 9
model.classification1$variable.importance
barplot(model.classification1$variable.importance,
col="lightpink",main = "variable Importance")
# Chunk 10
#test data process
data_test[, 1] <- ifelse(data_test[, 1] == -1, 0, 1)
selected_features_names <- colnames(selected_features)
data_test_new <- data_test[, selected_features_names]
testdata <- data.frame(
predict(pca, newdata = data_test_new)[,1:47],
Class = data_test[[1]])
testdata2 <- data.frame(
predict(pca, newdata = data_test_new)[,1:18],
Class = data_test[[1]])
testdata3 <- data.frame(data_test_new,Class = data_test[1])
str(testdata)
# Chunk 11
y_pred_pro <- predict(model.classification1,
newdata = testdata,
type = "prob")[,2]
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))
#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
print(conf_matrix)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
#ROC plot
perfd <- data.frame(x=perf@x.values[1][[1]],
y=perf@y.values[1][[1]])
ggplot(perfd,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") +
ylab("True positive rate") +
ggtitle(paste("Area under the curve:",
round(auc@y.values[[1]], 3)))
# Chunk 12
#bagging
set.seed(2023)
traindata3$Class <- as.factor(traindata3$Class)
bagging_Model <- randomForest(Class~.,
data=traindata3,
mtry=ncol(traindata2)-1,
ntree=1000)
bagging_Model
# Chunk 13
plot(bagging_Model$err.rate[,1],
type = "l",
xlab = "Number of trees",
ylab="OOB Error",
main="OBB Error vs. Number of Trees")
# Chunk 14
set.seed(2023)
bagging_Model <- randomForest(Class~.,data=traindata3,
mtry=ncol(traindata2)-1,
ntree=200)
bagging_Model
# Chunk 15
importance(bagging_Model)
varImpPlot(bagging_Model, main="predicting class")
# Chunk 16
y_pred_pro <- predict(bagging_Model,
newdata = testdata3,
type = "prob")[,2]
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))
#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
print(conf_matrix)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
#ROC plot
perfd <- data.frame(x=perf@x.values[1][[1]],
y=perf@y.values[1][[1]])
ggplot(perfd,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") +
ylab("True positive rate") +
ggtitle(paste("Area under the curve:",
round(auc@y.values[[1]], 3)))
# Chunk 17
#random forest
set.seed(2023)
random_Model <- randomForest(Class~.,data=traindata3,ntree=500)
random_Model
# Chunk 18
plot(random_Model$err.rate[,1],
type = "l",
xlab = "Number of trees",
ylab="OOB Error",
main="OBB Error vs. Number of Trees")
# Chunk 19
importance(random_Model)
varImpPlot(random_Model, main="predicting class")
# Chunk 20
y_pred_pro <- predict(random_Model,
newdata = testdata3,
type = "prob")[, 2]
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.numeric(as.vector(y_pred_pro))
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))
#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
print(conf_matrix)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
#ROC plot
perfd <- data.frame(x=perf@x.values[1][[1]],
y=perf@y.values[1][[1]])
ggplot(perfd,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") +
ylab("True positive rate") +
ggtitle(paste("Area under the curve:",
round(auc@y.values[[1]], 3)))
# Chunk 21
set.seed(83)
traindata3 <- data.frame(selected_features,Class = data_train[1])
testdata3 <- data.frame(data_test_new,Class = data_test[1])
nn_model <- neuralnet(Class ~ .,
data=testdata2,
hidden=c(533, 20),
act.fct = "logistic",
linear.output=FALSE)
#plotnet(nn_model)
# Chunk 22
y_pred_pro <- predict(nn_model,
newdata = testdata2,
type = "response")
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))
#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
print(conf_matrix)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
#ROC plot
perfd <- data.frame(x=perf@x.values[1][[1]],
y=perf@y.values[1][[1]])
ggplot(perfd,aes(x=x,y=y))+geom_line()+
xlab("False positive rate") +
ylab("True positive rate") +
ggtitle(paste("Area under the curve:",
round(auc@y.values[[1]], 3)))
