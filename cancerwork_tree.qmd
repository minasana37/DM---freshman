---
title: "Untitled"
format: html
editor: visual
---

## Data sets created

```{r}
data.train <- read.csv("C:\\Users\\10409\\Desktop\\Group 37\\group_37.csv")
data.test <- read.csv("C:\\Users\\10409\\Desktop\\Group 37\\arcene_test (for groups 32-38).csv")
data.train$Class<- as.factor(data.train$Class)
data.test$Class <- as.factor(data.test$Class)
```

## Dimension reduction

```{r}
#use PCA
train.pca  <- prcomp(data.train[,-1])
summary(train.pca)
plot(train.pca)
sd.pca <- summary(train.pca)$sdev
tot.pca <- sum(sd.pca^2)
ave.var <- tot.pca/(ncol(data.train)-1)
ave.var 
sd.pca^2 > ave.var
```

so we choose the PCS from 1 to 99.

```{r}
loading.matrix <- train.pca$rotation #matrix
#获取方差贡献率
variable_contribution <- colSums(train.pca$rotation[,1:99]^2)
cumulate_variance <- cumsum(sort(variable_contribution,decreasing = T))
threshold <- 0.95*sum(variable_contribution)
#选出贡献值95%的
selected_variance <- which(cumulate_variance <= threshold)
filtered_data <- data.train[,c(1,selected_variance+1)]
```

## Tree-based methods

### Classification Tree

```{r}
#classification
library(rpart)
library(rpart.plot)
set.seed(2023)
model.classification <- rpart(Class~.,data=filtered_data,method="class",cp=-1,minsplit=2,minbucket=1)
printcp(model.classification)
plotcp(model.classification)
```

#### Pruning a tree

```{r}
model.classification1 <- prune(model.classification,cp=0.08)
rpart.plot(model.classification1,type = 2,extra = 4)
```

#### Variable Importance

```{r}
model.classification1$variable.importance
barplot(model.classification1$variable.importance,col="lightpink",main = "variable Importance")
```

#### ROC and AUC:

```{r}
library(ROCR)
library(ggplot2)
pred.prob <- predict(model.classification1,newdata = data.test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data.test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
  xlab("False positive rate") + ylab("True positive rate") +
  ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
```

Prediction:

```{r}
class.pred <- predict(model.classification1,newdata = data.test[,-1],type="class")
test.table.class <- table(data.test[,1],class.pred)
test.table.class[1,1]/sum(test.table.class[1,])
test.table.class[2,2]/sum(test.table.class[2,])
```

### Bagging Tree

```{r}
library(randomForest)
#bagging
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=filtered_data,mtry=ncol(data.train)-1,ntree=1000)
Model.bagging
```

#### OOB:

```{r}
plot(Model.bagging$err.rate[,1],type = "l",
     xlab = "Number of trees",ylab="OOB Error",main="OBB Error vs. Number of Trees")
```

new model:

```{r}
set.seed(2023)
Model.bagging <- randomForest(Class~.,data=filtered_data,mtry=ncol(data.train)-1,ntree=900)
Model.bagging
```

#### Variable Importance:

```{r}
varImpPlot(Model.bagging, main="predicting class")
```

ROC and AUC:

```{r}
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.bagging,newdata = data.test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data.test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
  xlab("False positive rate") + ylab("True positive rate") +
  ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
```

Prediction

```{r}
bagg.pred <- predict(Model.bagging,newdata = data.test[,-1],type="class")
test.table.bagg <- table(data.test[,1],bagg.pred)
test.table.bagg[1,1]/sum(test.table.bagg[1,])
test.table.bagg[2,2]/sum(test.table.bagg[2,])
```

### Random Forests

```{r}
#random forest
set.seed(2023)
Model.random <- randomForest(Class~.,data=filtered_data,ntree=500)
Model.random
```

OBB(out of bag)

```{r}
plot(Model.random$err.rate[,1],type = "l",
     xlab = "Number of trees",ylab="OOB Error",main="OBB Error vs. Number of Trees")

```

Variable Importance:

```{r}
varImpPlot(Model.random, main="predicting class")
```

Prediction:

```{r}
random.pred <- predict(Model.random,newdata = data.test[,-1],type="class")
test.table.random <- table(data.test[,1],random.pred)
test.table.random[1,1]/sum(test.table.random[1,])
test.table.random[2,2]/sum(test.table.random[2,])
```

AUC:

```{r}
library(ROCR)
library(ggplot2)
pred.prob <- predict(Model.random,newdata = data.test,type = "prob")[,2]
true_labels <- as.numeric(as.character(data.test$Class))
true_labels[true_labels == -1] <- 0
score2 <- prediction(pred.prob,true_labels)
perf2 <- performance(score2,"tpr","fpr")
auc2 <- performance(score2,"auc")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],y=perf2@y.values[1][[1]])
ggplot(perfd2,aes(x=x,y=y))+geom_line()+
  xlab("False positive rate") + ylab("True positive rate") +
  ggtitle(paste("Area under the curve:", round(auc2@y.values[[1]], 3)))
```
