---
title: "Analysis of Cancer Data"
author: "Group_37"
format: 
  html:
    embed-resources: true
    code-tools: true
  pdf: 
    include-in-header: 
      text: |
        \usepackage{booktabs}
        \usepackage{float}
        \floatplacement{table}{H}
        \usepackage{setspace}  
        \onehalfspacing        
        \usepackage{geometry}  
        \geometry{left=1in, right=1in, top=1in, bottom=1in} 
    latex_engine: xelatex
    fontsize: 12pt  
editor_options: 
  chunk_output_type: console
execute:
  echo: false
  eval: true
  warning: false
  message: false
pdf_document: 
    latex_engine: xelatex
    mainfont: "Times New Roman" 
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{graphicx}
  - \usepackage{geometry}
html_document:
    mathjax: true
editor: 
  markdown: 
    wrap: sentence
---

## Data sets created

```{r}
#| warning: false
#packages
library(dplyr) 
library(neuralnet)
library(NeuralNetTools)
library(pROC)
library(rpart)
library(rpart.plot)
library(ROCR)
library(ggplot2)
library(randomForest)
library(MASS)
library(biotools)
library(magick)
```

# Introduction

Modern mass spectrometry collects thousands of molecular features, but analyzing such high-dimensional data is challenging.
Identifying key patterns can aid early diagnosis and improve treatment.

To explore the question, the Arcene dataset will be divided into a training set and a test set.
A unique training set containing 100 samples with 5,000 randomly selected features.
A fixed test set (100 samples with all 10,000 features) will be used for model evaluation.
The study will employ various classification techniques to assess their accuracy in distinguishing between cancerous and normal tissue samples.
Through the model analysis, to explore the following research questions:

-   **Primary research question**: Can biochemical features accurately distinguish between cancerous and normal tissue samples?

-   **Secondary research question**: Compare the results of different classification models to find out a best classification model.

# Data Processing

```{r}
#| results: 'hide'
data_train <- read.csv("group_37.csv")
data_test <- read.csv("arcene_test (for groups 32-38).csv")
sum(is.na(data_train))
sum(is.na(data_test))
ncol(data_train) == 
  sum(colnames(data_train) %in% colnames(data_test))
class <- data_train[, 1]
features <- data_train[, -1]
data_train[, 1] <- ifelse(data_train[, 1] == -1, 0, 1)
```

## Initial rejection of potential probes

Based on the dataset description, we performed an initial feature selection process to reduce the dimensionality of the data and eliminate variables that may not provide meaningful insights for the model.
We focused on removing features with extremely low variance, as these are typically considered to be probes or noise sequences that do not contribute valuable information for predictive modeling.
Such features often show minimal variability across observations and do not offer distinguishing power for classification or regression tasks.
By removing these low-variance features, we reduced the data dimensionality, which facilitates more efficient analysis and model training, and helps to prevent overfitting by removing redundant or irrelevant information.
This step is essential for improving the overall performance and interpretability of the model.

```{r}
#| echo: false
#| label: fig-init1
#| fig-cap: Variable Standard Deviation Distribution
#| fig.width: 6     
#| fig.height: 4 
feature_sd <- apply(features, 2, sd)
par(mfrow = c(1, 2))
boxplot(feature_sd, 
        xlab = "Features", 
        ylab = "Standard Deviation", 
        col = "lightblue")
hist(feature_sd, 
     xlab = "Standard Deviation", 
     ylab = "Frequency", 
     main = " ",
     col = "lightblue", 
     border = "black", 
     breaks = 100)
```

```{r}
#| results: 'hide'
q1 <- quantile(feature_sd, 0.25)  
q3 <- quantile(feature_sd, 0.75) 
sd_threshold <- q3-q1
selected_features <- features[, feature_sd > sd_threshold]
removed_features <- features[, feature_sd <= sd_threshold]
cat("remaing features", ncol(selected_features), "\n")
cat("removed features:", ncol(removed_features), "\n")
```

## Data Dimension Reduction Processing

PCA was further applied to reduce the dimensionality of the data.
The Kaiser criterion and the cumulative variance contribution ratio were used to select the effective principal components.
Based on these criteria, the original dataset was updated, leading to a significant reduction in its dimensionality.

```{r}
#| results: 'hide'
#use PCA
pca<-prcomp(selected_features)
sd.pca <- pca$sdev
#based on kaiser
tot.var<-sum(sd.pca^2)
ave.var<-tot.var/ncol(selected_features)
ave.var 
sd.pca^2 > ave.var
all_variance <- sd.pca^2
cum_variance <- cumsum(all_variance) / sum(all_variance)
selected_components <- which(sd.pca^2 > ave.var)
selected_variance <- sum(all_variance[selected_components]) / sum(all_variance)
cat("Cumulative contribution:", selected_variance, "\n")
pca_features1 <- pca$x[, 1:47]
```

```{r}
#| results: 'hide'
#based on Cumulative variance contribution 95%
cumulative_variance <- cumsum(sd.pca^2) / sum(sd.pca^2)
selected_components_cumulative <- which(cumulative_variance >= 0.95)[1]
selected_components_cumulative
pca_features2 <- pca$x[, 1:selected_components_cumulative]
```

```{r}
#| results: 'hide'
traindata <- data.frame(pca_features1, Class = data_train[[1]])
traindata2 <- data.frame(pca_features2, Class = data_train[[1]])
traindata3 <- data.frame(selected_features,Class = data_train[1])
data_train$Class<- as.factor(data_train$Class)
#str(traindata)
```

# Formal data analysis

## Tree-based methods

### Classification Tree

Classification Tree partition the feature space into a number of disjoint and non-overlapping regions.
And predict the class of a given observation as the most commonly occurring class of training observations is the region to which it belongs.

```{r}
#| echo: false
#| results: 'hide'
#| warning: false
#| label: fig-tree1
#| fig-cap: Xerror vs CP
set.seed(2023)
par(mfrow = c(1, 1))
model.classification <- rpart(Class~.,
                              data=traindata,
                              method="class",
                              cp=-1,
                              minsplit=2,
                              minbucket=1)
plotcp(model.classification)
```

```{r}
#printcp(model.classification)
```

The pruning process is based on the complexity parameter (cp) selection.
The optimal cp is chosen as the largest value within the range where the cross-validation error remains within one standard deviation of the minimum error.
Based on this criterion, a cp of 0.032 is selected to prune the new tree.

```{r}
#| echo: false
#| results: 'hide'
#| label: fig-tree2
#| fig-cap: Pruned Classification Tree (cp=0.032)
#| fig.width: 6      
#| fig.height: 4 
par(mfrow = c(1, 2))
model.classification1 <- prune(model.classification,cp=0.032)
rpart.plot(model.classification1,type = 2,extra = 4)
barplot(model.classification1$variable.importance,
        col = "lightpink",
        main  = "Variable-Importance",
        xlab = "Variables",  
        ylab = "Importance") 
```

The classification tree also highlights the importance of variables, showing that PC1 is the most significant, followed by PC12 and PC2.
Variables like PC18, PC13, PC21, and PC24 are considered less important.

```{r}
#| results: 'hide'
#test data process
data_test[, 1] <- ifelse(data_test[, 1] == -1, 0, 1)
selected_features_names <- colnames(selected_features)
data_test_new <- data_test[, selected_features_names]
testdata <- data.frame(
              predict(pca, newdata = data_test_new)[,1:47],
              Class = data_test[[1]])
testdata2 <- data.frame(
              predict(pca, newdata = data_test_new)[,1:18],
              Class = data_test[[1]])
testdata3 <- data.frame(data_test_new,Class = data_test[1])
#str(testdata)
```

```{r}
#| results: 'hide'
#| echo: false
#| warning: false
#| message: false
y_pred_pro <- predict(model.classification1,
                     newdata = testdata,
                     type = "prob")[,2]
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))
#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
print(conf_matrix)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))

```

The test accuracy is 0.68, indicating poor performance.
While the classification tree has some predictive ability, its overall effectiveness is limited.

### Bagging Tree

The core idea of bagging is to repeatedly draw samples from the original dataset and build a classification tree on each bootstrapped sample.
For each test observation, the class predicted by each tree is recorded.
The final prediction is determined by a majority vote, where the class that appears most frequently across all predictions becomes the overall predicted class.
To select the minimum number of trees that stabilize the OOB error, the model was trained with different numbers of trees, and the OOB error was monitored.
Once the error stabilized, the smallest number of trees that achieved this stabilization was chosen to build the final Bagging Tree model.

```{r}
#| results: 'hide'
#bagging
set.seed(2023)
traindata3$Class <- as.factor(traindata3$Class)
bagging_Model <- randomForest(Class~.,
                             data=traindata3,
                             mtry=ncol(traindata2)-1,
                             ntree=1000)
```

```{r}
#| echo: false
#| results: 'hide'
#| warning: false
#| label: fig-tree5
#| fig-cap: Model Performance 
#| fig.width: 10      
#| fig.height: 7 
par(mfrow = c(1, 2))
plot(bagging_Model$err.rate[,1],
     type = "l",
     xlab = "Number of trees",
     ylab="OOB Error",
     main="OBB Error vs. Number of Trees")
set.seed(2023)
bagging_Model <- randomForest(Class~.,data=traindata3,
                              mtry=ncol(traindata2)-1,
                              ntree=200)
varImpPlot(bagging_Model, main="Variable Importance")
```

According to the Bagging Tree, Variable.2640 is the most important factor, followed by Variable.4192 and Variable.6481, Variable.132 and Variable.1193, Variable.6442 and Variable.1235 are relatively unimportant.The Accuracy of test is 0.79, which is good.
Showing the Bagging Tree has good classification ability.

```{r}
#| results: 'asis'
#| echo: false
#| warning: false
#| message: false
y_pred_pro <- predict(bagging_Model,
                     newdata = testdata3,
                     type = "prob")[,2]
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))
#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))

```

### Random Forests

Random forests enhance Bagging Tree by reducing the correlation between individual trees.
By randomly excluding a subset of variables at each split, the trees become more diverse, leading to more stable and reliable predictions.

```{r}
#random forest
set.seed(2023)
random_Model <- randomForest(Class~.,data=traindata3,ntree=500)
```

```{r}
#| echo: false
#| results: 'hide'
#| warning: false
#| label: fig-tree8
#| fig-cap: Model Performance 
#| fig.width: 10      
#| fig.height: 7 
par(mfrow = c(1, 2))
plot(random_Model$err.rate[,1],
     type = "l",
     xlab = "Number of trees",
     ylab="OOB Error",
     main="OBB Error vs. Number of Trees")
varImpPlot(random_Model, main="predicting class")
```

According to the random forests, Variable.2640 and Variable.4192, which were also important in the Bagging model, remain influential.

```{r}
#| echo: false
#| results: 'asis'
y_pred_pro <- predict(random_Model,
                     newdata = testdata3,
                     type = "prob")[,2]
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))
#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
```

```{r}
#| results: 'hide'
#| echo: false
#| warning: false
#| message: false
#| label: fig-tree10
#| fig-cap: ROC Plot Compare
#| fig.width: 6      
#| fig.height: 4 
par(mfrow = c(1, 1))
y_pred_pro1 <- predict(model.classification1,
                     newdata = testdata,
                     type = "prob")[,2]
y_pred_pro2 <- predict(bagging_Model,
                     newdata = testdata3,
                     type = "prob")[,2]
y_pred_pro3 <- predict(random_Model, 
                      newdata = testdata3, 
                      type = "prob")[, 2]
score1 <- ROCR::prediction(y_pred_pro1, y_true)
perf1 <- performance(score1,"tpr","fpr")
perfd1 <- data.frame(x=perf1@x.values[1][[1]],
                    y=perf1@y.values[1][[1]])
score2 <- ROCR::prediction(y_pred_pro2, y_true)
perf2 <- performance(score2,"tpr","fpr")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],
                    y=perf2@y.values[1][[1]])
score3 <- ROCR::prediction(y_pred_pro3, y_true)
perf3 <- performance(score3,"tpr","fpr")
perfd3 <- data.frame(x=perf3@x.values[1][[1]],
                    y=perf3@y.values[1][[1]])
perfd1$model <- "classification tree"
perfd2$model <- "bagging tree"
perfd3$model <- "random forest"
roc_data <- rbind(perfd1, perfd2,perfd3)
ggplot(roc_data, aes(x = x, y = y, color = model)) +
  geom_line(size = 1.2) + 
  xlab("False Positive Rate") + 
  ylab("True Positive Rate") +
  ggtitle("ROC Curve Comparison") +
  scale_color_manual(values = c("skyblue", "pink", "darkorange")) +  
  theme_minimal() +
  theme(legend.position = "right")
```

The ROC comparison shows that the AUC values for the classification tree, bagging tree, and random forest are 0.6936, 0.8953, and 0.9111, respectively.
Since the random forest achieves the highest AUC, it demonstrates the best predictive performance among the three models.

## Discriminant Analysis

### Test of boxm

The Box's M-test for homogeneity of covariance matrices was performed, with the results showing a Chi-Square value of 576.49 and degrees of freedom (df) equal to 171.
The p-value was extremely small (\< 2.2e-16), indicating that the assumption of homogeneity of covariance matrices is violated.
However, I still proceeded with LDA as a comparison method.

Next, the Shapiro-Wilk normality test was conducted for each feature, and the Bonferroni correction was applied to adjust for multiple comparisons.
A total of 5 features failed the normality test after the correction, meaning they do not follow a normal distribution.

```{r}
#| results: 'hide'
#| echo: false
boxm <- boxM(traindata[,-ncol(traindata)], traindata$Class)
print(boxm)
shapiro.test(traindata[,2])  
shapiro.test(traindata[,3])
```

```{r}
#| results: 'hide'
#| echo: false
###LDA
class_lda <- lda(Class~., data = traindata)
class_pred_lda <- predict(class_lda, testdata)

acc_lda <- mean(class_pred_lda$class == testdata$Class)
cat("acc:", acc_lda, "\n")
table(class_pred_lda$class, testdata$Class)

```

```{r}
#| results: 'hide'
#| echo: false
### QDA
class_qda <- qda(Class~., data = traindata2)
class_pred_qda <- predict(class_qda, testdata2)
acc_qda <- mean(class_pred_qda$class == testdata2$Class)
cat("acc:", acc_qda, "\n")
table(class_pred_qda$class, testdata2$Class)

```

Two classification models, Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA), were then applied to the data.
For LDA, the accuracy was 73%, with a confusion matrix showing 45 true negatives, 28 true positives, 11 false positives, and 16 false negatives.
For QDA, the accuracy improved to 79%, with a confusion matrix showing 48 true negatives, 31 true positives, 8 false positives, and 13 false negatives.
Finally, ROC curves were plotted for both LDA and QDA.
The AUC (Area Under the Curve) values were printed on the plots, providing a visual representation of the models' performance.
The AUC for QDA was superior to that of LDA, indicating that QDA had better discriminatory power in distinguishing between the two classes.

```{r}
#| label: ROC of LDA and QDA
#| fig-cap: ROC 
#| fig.width: 8      
#| fig.height: 4 
par(mfrow = c(1, 2))
roc_lda <- roc(testdata$Class, class_pred_lda$posterior[,2])
plot(roc_lda, main = "ROC for LDA", print.auc = T, auc.polygon = T, legacy.axes = T)

roc_qda <- roc(testdata$Class, class_pred_qda$posterior[,2])
plot(roc_qda, main = "ROC for QDA", print.auc = T, auc.polygon = T, legacy.axes = T)

```

The histograms show that both LDA and QDA assign most observations with probabilities close to 0 or 1, indicating high confidence in their classifications.
This could suggest that the models are performing well, but it might also imply overfitting or sensitivity to the training data.

```{r}
#| label: Distribution of Posterior
#| fig-cap: Distribution of Posterior Probabilities 
#| fig.width: 8      
#| fig.height: 4 
par(mfrow = c(1, 2))
hist(class_pred_lda$posterior[,2], main = "Distribution of Posterior Probabilities for LDA", xlab = "Probability")
hist(class_pred_qda$posterior[,2], main = "Distribution of Posterior Probabilities for QDA", xlab = "Probability")
```

## nn work

Neural networks improve predictive performance by capturing complex nonlinear relationships between features and the target variable.
With multiple hidden layers and structured activation functions, they learn intricate patterns in the data.
Additionally, their architecture reduces dependence on any single feature, enhancing generalization.

```{r}
#| results: 'hide'
#| echo: false
traindata3 <- data.frame(selected_features,Class = data_train[1])
testdata3 <- data.frame(data_test_new,Class = data_test[1])
```

Based on the PCA results, we initially designed a neural network with two hidden layers: the first layer containing 47 neurons and the second layer containing 20 neurons.
This structure was chosen to capture the primary features of the dataset while maintaining a balance between model complexity and computational efficiency.
The aim was to leverage the dimensionality reduction insights from PCA to guide the architecture design and improve the model's ability to extract meaningful patterns from the data.

```{r}
#| results: 'hide'
#| echo: false
set.seed(82)
nn_model1 <- neuralnet(Class ~ .,
                       data=testdata3, 
                       hidden = c(47, 20), 
                       act.fct = "logistic",
                       linear.output=FALSE)
```

```{r}
#| results: 'asis'
#| echo: false
y_pred_pro <- predict(nn_model1, 
                            newdata = testdata3, 
                            type = "response")
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))

#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
```

The training performance was suboptimal, and the model did not meet expectations.
One possible reason for this was the insufficient network capacity, which prevented the model from fully capturing the complex patterns within the data.

##Adjustment Strategy To improve the model's learning ability, we increased the number of neurons in the first hidden layer to 533, enhancing its capability to extract meaningful features.
In the subsequent hidden layers, we gradually reduced the number of neurons to 256 → 60 → 20, aiming to refine the model's representation while preventing overfitting.
This hierarchical structure allows the network to capture high-dimensional features in the initial layers and progressively distill the most relevant information in the deeper layers.

```{r}
#| results: 'hide'
#| echo: false
set.seed(83)
nn_model2 <- neuralnet(Class ~ .,
                       data=testdata3, 
                       hidden = c(533, 256, 60, 20), 
                       act.fct = "logistic",
                       linear.output=FALSE)
#plotnet(nn_model)
```

The training performance improved significantly, with the AUC increasing from 0.64 to 0.95.
This dramatic improvement suggests that the adjusted network architecture effectively enhanced the model’s ability to learn complex patterns, leading to a much better classification performance.
The increased network capacity in the initial layers allowed for better feature extraction, while the gradual reduction in neurons helped refine the representations, ultimately resulting in a more robust and well-generalized model.

```{r}
#| results: 'asis'
#| echo: false
y_pred_pro <- predict(nn_model2, 
                      newdata = testdata3, 
                      type = "prob")
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))

#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
```

```{r}
#| label: nnROC
#| fig-cap: ROC of nn1 and nn2
#| fig.width: 4      
#| fig.height: 4 
par(mfrow = c(1, 1))
y_pred_pro1 <- predict(nn_model1, 
                      newdata = testdata3, 
                      type = "prob")
y_pred_pro2 <- predict(nn_model2, 
                      newdata = testdata3, 
                      type = "prob")

score1 <- ROCR::prediction(y_pred_pro1, y_true)
perf1 <- performance(score1,"tpr","fpr")
perfd1 <- data.frame(x=perf1@x.values[1][[1]],
                    y=perf1@y.values[1][[1]])

score2 <- ROCR::prediction(y_pred_pro2, y_true)
perf2 <- performance(score2,"tpr","fpr")
perfd2 <- data.frame(x=perf2@x.values[1][[1]],
                    y=perf2@y.values[1][[1]])

perfd1$model <- "nn_model1"
perfd2$model <- "nn_model2"


roc_data <- rbind(perfd1, perfd2)


ggplot(roc_data, aes(x = x, y = y, color = model)) +
  geom_line() + 
  xlab("False Positive Rate") + 
  ylab("True Positive Rate") +
  ggtitle("ROC Curve Comparison") +
  scale_color_manual(values = c("blue", "red")) +  
  theme_minimal() +
  theme(legend.position = "bottom")

```

## SVM

The datasets contain features derived from different preprocessing methods:

-   Dataset 1: Original dataset with 5000 features.

-   Dataset 2: Feature selection reduced the number of features to 1824.

-   Dataset 3: Principal Component Analysis (PCA) reduced the feature count to 47.

For each dataset, SVM models were trained, tested, and optimized using GridSearchCV to identify the best hyperparameters.
The datasets were loaded and divided into features (X) and target labels (y).
Standardization was applied using StandardScaler to ensure the SVM operates effectively.
An initial SVM model with default parameters was trained on each dataset.
Model performance was evaluated using accuracy and Area Under the Curve (AUC) scores.
Hyperparameter tuning was performed using GridSearchCV to optimize SVM parameters (C, gamma, and kernel).
The best model from the grid search was selected and evaluated on the test set.
The following hyperparameters were explored: C: \[0.01, 0.1, 0.5, 1, 2, 10, 100\], gamma: \["scale", "auto", 0.01, 0.1, 1\], kernel: \["linear", "rbf", "poly"\].
The best hyperparameters were selected based on AUC scores.

| Dataset           | Features | Accuracy | AUC   |
|-------------------|----------|----------|-------|
| Original          | 5000     | 0.84     | 0.930 |
| Feature selection | 1824     | 0.88     | 0.948 |
| PCA Reduction     | 47       | 0.81     | 0.924 |

:::::: columns
::: {.column width="33%" align="left"}
![](svm1.png){width="100%"}
:::

::: {.column width="33%" align="center"}
![](svm2.png){width="100%"}
:::

::: {.column width="33%" align="right"}
![](svm3.png){width="100%"}
:::
::::::

Reducing the number of features from 5000 to 1824 improved accuracy and AUC after tuning.
PCA-reduced data (47 features) maintained good accuracy but performed slightly worse than feature selection.
In most cases, GridSearchCV improved AUC over the default model.
For Dataset 1, a linear kernel was chosen as optimal, while for Datasets 2 and 3, an RBF kernel was preferred. 
 

Feature selection (1800 features) provided the best performance in terms of both accuracy and AUC.
PCA (40 features) led to minor accuracy reduction but is still a viable option for dimensionality reduction.

## Conclusion:

| Classification Model | Accuracy | AUC    |
|----------------------|----------|--------|
| Classification tree  | 0.68     | 0.6936 |
| Bagging tree         | 0.79     | 0.8953 |
| Random forests       | 0.81     | 0.9111 |
| LQA                  | 0.73     | 0.908  |
| QDA                  | 0.79     | 0.902  |
| NN works             | 0.89     | 0.95   |
| SVM                  | 0.88     | 0.948  |
