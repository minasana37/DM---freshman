---
title: "Analysis of Gen Cancer Data"
author: "Group_37"
format: 
  html:
    embed-resources: true
    code-tools: true
  pdf: 
    include-in-header: 
      text: |
        \usepackage{booktabs}
        \usepackage{float}
        \floatplacement{table}{H}
editor_options: 
  chunk_output_type: console
execute:
  echo: false
  eval: true
  warning: false
  message: false
pdf_document: 
    latex_engine: xelatex
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{graphicx}
  - \usepackage{geometry}
html_document:
    mathjax: true
---

## Data sets created

```{r}
#packages
library(dplyr) 
library(neuralnet)
library(NeuralNetTools)
library(pROC)
library(rpart)
library(rpart.plot)
library(ROCR)
library(ggplot2)
library(randomForest)
library(MASS)
library(biotools)
```

# Introduction

# Research Question


# Data Processing
```{r}
data_train <- read.csv("group_37.csv")
data_test <- read.csv("arcene_test (for groups 32-38).csv")
sum(is.na(data_train))
sum(is.na(data_test))
ncol(data_train) == 
  sum(colnames(data_train) %in% colnames(data_test))
class <- data_train[, 1]
features <- data_train[, -1]
data_train[, 1] <- ifelse(data_train[, 1] == -1, 0, 1)
```

## Initial rejection of potential probes
```{r}
feature_sd <- apply(features, 2, sd)
boxplot(feature_sd)
q1 <- quantile(feature_sd, 0.25)  
q3 <- quantile(feature_sd, 0.75) 
sd_threshold <- q3-q1
selected_features <- features[, feature_sd > sd_threshold]
removed_features <- features[, feature_sd <= sd_threshold]
cat("remaing features", ncol(selected_features), "\n")
cat("removed features:", ncol(removed_features), "\n")
```

## Data Dimension Reduction Processing
```{r}
#use PCA
pca<-prcomp(selected_features)
sd.pca <- pca$sdev
#based on kaiser
tot.var<-sum(sd.pca^2)
ave.var<-tot.var/ncol(selected_features)
ave.var 
sd.pca^2 > ave.var
all_variance <- sd.pca^2
cum_variance <- cumsum(all_variance) / sum(all_variance)
selected_components <- which(sd.pca^2 > ave.var)
selected_variance <- sum(all_variance[selected_components]) / sum(all_variance)
cat("Cumulative contribution:", selected_variance, "\n")
pca_features1 <- pca$x[, 1:47]
```


```{r}
#based on Cumulative variance contribution 95%
cumulative_variance <- cumsum(sd.pca^2) / sum(sd.pca^2)
selected_components_cumulative <- which(cumulative_variance >= 0.95)[1]
selected_components_cumulative
pca_features2 <- pca$x[, 1:selected_components_cumulative]
```

```{r}
traindata <- data.frame(pca_features1, Class = data_train[[1]])
traindata2 <- data.frame(pca_features2, Class = data_train[[1]])
traindata3 <- data.frame(selected_features,Class = data_train[1])
data_train$Class<- as.factor(data_train$Class)
str(traindata)
```

## Tree-based methods

### Classification Tree
Description: Classification Tree partition the feature space into a number of disjoint and non-overlapping regions. And predict the class of a given observation as the most commonly occurring class of training observations is the region to which it belongs.

Reason: it's easy to understand and handle the categorical features without the need to create a long series of dummy variables.

```{r}
set.seed(2023)
model.classification <- rpart(Class~.,
                              data=traindata,
                              method="class",
                              cp=-1,
                              minsplit=2,
                              minbucket=1)
printcp(model.classification)
plotcp(model.classification)
```
Start with a tree is fully grown, to see the cross validation results use the printcp() function and use plotcp() function to check the complexity parameter value. The smallest tree strategy refers to the largest cp value which is under the dashed line; the intercept of this line equals to the minimum xerror plus its standard deviation 0.642514(0.54545+ 0.097064). Check the table, the value in the range of (0.045455, 0.068182), from the cp plot, the table should larger than 0.642514. So use the cp is 0.032 to prune the tree.

#### Pruning a tree

```{r}
model.classification1 <- prune(model.classification,cp=0.032)
rpart.plot(model.classification1,type = 2,extra = 4)
```
After pruning trees with cp =0.032. Check the plot of tree, the variable where the split happens are PC1, PC12, PC13,PC29,PC19,PC31. The terminal nodes of the tree with the 1 as the predicted class have a high probability and the terminal nodes of the tree with 0(-1) as the predicted class have a high probability.

#### Variable Importance

```{r}
model.classification1$variable.importance
barplot(model.classification1$variable.importance,
        col="lightpink",main = "variable Importance")
```
According to the classification tree, PC1 is the most important factor, followed by PC12 and PC2, then the PC18 and PC13, PC21 and PC24 are relatively unimportant.

#### ROC and AUC:

```{r}
#test data process
data_test[, 1] <- ifelse(data_test[, 1] == -1, 0, 1)
selected_features_names <- colnames(selected_features)
data_test_new <- data_test[, selected_features_names]
testdata <- data.frame(
              predict(pca, newdata = data_test_new)[,1:47],
              Class = data_test[[1]])
testdata2 <- data.frame(
              predict(pca, newdata = data_test_new)[,1:18],
              Class = data_test[[1]])
testdata3 <- data.frame(data_test_new,Class = data_test[1])
str(testdata)
```


```{r}
y_pred_pro <- predict(model.classification1,
                     newdata = testdata,
                     type = "prob")[,2]
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))

#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
print(conf_matrix)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
#ROC plot
perfd <- data.frame(x=perf@x.values[1][[1]],
                    y=perf@y.values[1][[1]])
ggplot(perfd,aes(x=x,y=y))+geom_line()+
       xlab("False positive rate") + 
       ylab("True positive rate") +
ggtitle(paste("Area under the curve:", 
              round(auc@y.values[[1]], 3)))

```

The ROC curve show the classification performance. A higher AUC(close to 1 ) indicates a better-performing model. The AUC of classification tree is 0.694, which is greater than 0.5, show the classification effect is fair.


Prediction:

After the predict of the test set, obtained the Confusion Matrix and the Accuracy. The Accuracy of test set is 0.68, which is poor. The classification tree model has some classification ability, but it is not particularly good.

### Bagging Tree
Description: Repeatedly draw samples from the original dataset and build a classification tree on each bootstrapped sample. For a given test observation, record the class predicted from each tree and take a majority vote: the overall prediction is the most commonly occurring class across all the predictions.

Reason: Reduce the overfitting risk of a single tree, improve the stability and performance of the model. Can automatically adjust feature importance and easy to explain.

Set the dataset after the feature selection:
```{r}
#bagging
set.seed(2023)
traindata3$Class <- as.factor(traindata3$Class)
bagging_Model <- randomForest(Class~.,
                             data=traindata3,
                             mtry=ncol(traindata2)-1,
                             ntree=1000)
bagging_Model
```
Start with randomForest() function and specify that the number of variables tried at each split,mtry,should be equal to the number of variables in the model. Set the ntree= 500, check the out-of-bag estimate of error rate.
#### OOB:

```{r}
plot(bagging_Model$err.rate[,1],
     type = "l",
     xlab = "Number of trees",
     ylab="OOB Error",
     main="OBB Error vs. Number of Trees")
```
According to the graph, in the range of (200,500), the OOB error starts to stabilize after decreasing. Select the minimum number of trees that can stabilize the OBB error, there choose ntree = 200.

new model:

```{r}
set.seed(2023)
bagging_Model <- randomForest(Class~.,data=traindata3,
                              mtry=ncol(traindata2)-1,
                              ntree=200)
bagging_Model
```

Building a new bagging forest model with ntree =200.

#### Variable Importance:

```{r}
importance(bagging_Model)
varImpPlot(bagging_Model, main="predicting class")
```
According to the bagging forest, Variable.2640 is the most important factor, followed by Variable.4192 and Variable.6481, Variable.132 and Variable.1193, Variable.6442 and Variable.1235 are relatively unimportant.

ROC and AUC:

```{r}
y_pred_pro <- predict(bagging_Model,
                     newdata = testdata3,
                     type = "prob")[,2]
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.vector(y_pred_pro)
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))

#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
print(conf_matrix)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
#ROC plot
perfd <- data.frame(x=perf@x.values[1][[1]],
                    y=perf@y.values[1][[1]])
ggplot(perfd,aes(x=x,y=y))+geom_line()+
       xlab("False positive rate") + 
       ylab("True positive rate") +
ggtitle(paste("Area under the curve:", 
              round(auc@y.values[[1]], 3)))
```

Prediction:
After the predict of the test set, obtained the Confusion Matrix and the Accuracy. The Accuracy of test set is 0.79, which is good. The bagging forest model has good classification ability.


### Random Forests
Description: Random forests improve upon bagging by decorrelating the individual trees. By forcibly excluding a random subset of variables, the correlation between any pair of trees is reduced. Therefore the average predictions will be more reliable.

Reason: Reduce the overfitting risk of a single tree, avoid strong feature influence ,improve the stability and performance of the model, make the average predictions will be more reliable.

```{r}
#random forest
set.seed(2023)
random_Model <- randomForest(Class~.,data=traindata3,ntree=500)
random_Model
```

OBB(out of bag)

```{r}
plot(random_Model$err.rate[,1],
     type = "l",
     xlab = "Number of trees",
     ylab="OOB Error",
     main="OBB Error vs. Number of Trees")

```

Variable Importance:

```{r}
importance(random_Model)
varImpPlot(random_Model, main="predicting class")
```

According to the random forests, Variable.6481 is the most important factor, followed by Variable.2640 and Variable.4192, Variable.3986and Variable.471, Variable.2435and Variable.6215 are relatively unimportant.

```{r}
y_pred_pro <- predict(random_Model, 
                      newdata = testdata3, 
                      type = "prob")[, 2]
y_pred_pro <- unname(y_pred_pro)
y_pred_pro <- as.numeric(as.vector(y_pred_pro))
y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
y_true <- as.numeric(as.character(testdata$Class))

#Accuracy
accuracy <- sum(y_pred == y_true) / length(y_true)
print(paste("Accuracy:", accuracy))
#conf_matrix
conf_matrix <- table(Predicted = y_pred, Actual = y_true)
print(conf_matrix)
#AUC
score <- ROCR::prediction(y_pred_pro, y_true)
auc <- performance(score, "auc")
perf <- performance(score,"tpr","fpr")
print(paste("AUC:", auc@y.values[[1]]))
#ROC plot
perfd <- data.frame(x=perf@x.values[1][[1]],
                    y=perf@y.values[1][[1]])
ggplot(perfd,aes(x=x,y=y))+geom_line()+
       xlab("False positive rate") + 
       ylab("True positive rate") +
ggtitle(paste("Area under the curve:", 
              round(auc@y.values[[1]], 3)))
```

The ROC curve show the classification performance. A higher AUC(close to 1 ) indicates a better-performing model. The AUC of bagging forest is 0.911, which is greater than 0.5 and close to 1, show the classification effect is excellent.

After the predict of the test set, obtained the Confusion Matrix and the Accuracy. The Accuracy of test set is 0.81, which is good. The bagging forest model has good classification ability.

### nn work
```{r}
set.seed(83)
traindata3 <- data.frame(selected_features,Class = data_train[1])
testdata3 <- data.frame(data_test_new,Class = data_test[1])
nn_model <- neuralnet(Class ~ .,
                       data=testdata3, 
                       hidden=c(533, 20), 
                       act.fct = "logistic",
                       linear.output=FALSE)
#plotnet(nn_model)
```

```{r}
# y_pred_pro <- predict(nn_model, 
#                             newdata = testdata2, 
#                             type = "response")
# y_pred_pro <- unname(y_pred_pro)
# y_pred_pro <- as.vector(y_pred_pro)
# y_pred <- ifelse(y_pred_pro > 0.5, 1, 0)
# y_true <- as.numeric(as.character(testdata$Class))
# 
# #Accuracy
# accuracy <- sum(y_pred == y_true) / length(y_true)
# print(paste("Accuracy:", accuracy))
# #conf_matrix
# conf_matrix <- table(Predicted = y_pred, Actual = y_true)
# print(conf_matrix)
# #AUC
# score <- ROCR::prediction(y_pred_pro, y_true)
# auc <- performance(score, "auc")
# perf <- performance(score,"tpr","fpr")
# print(paste("AUC:", auc@y.values[[1]]))
# #ROC plot
# perfd <- data.frame(x=perf@x.values[1][[1]],
#                     y=perf@y.values[1][[1]])
# ggplot(perfd,aes(x=x,y=y))+geom_line()+
#        xlab("False positive rate") + 
#        ylab("True positive rate") +
# ggtitle(paste("Area under the curve:", 
#               round(auc@y.values[[1]], 3)))
```

## Discriminant Analysis

### Test of boxm

```{r}


boxm <- boxM(traindata2[,-ncol(traindata2)], traindata2$Class)
print(boxm)

p_values <- sapply(traindata2[, -ncol(traindata2)], function(column) shapiro.test(column)$p.value)
alpha <- 0.05
bonferroni_threshold <- alpha / length(p_values)
rejected_normality <- p_values < bonferroni_threshold
sum(rejected_normality)

```

The Box's M-test for homogeneity of covariance matrices was performed, with the results showing a Chi-Square value of 576.49 and degrees of freedom (df) equal to 171. The p-value was extremely small (< 2.2e-16), indicating that the assumption of homogeneity of covariance matrices is violated. However, I still proceeded with LDA as a comparison method.

Next, the Shapiro-Wilk normality test was conducted for each feature, and the Bonferroni correction was applied to adjust for multiple comparisons. A total of 5 features failed the normality test after the correction, meaning they do not follow a normal distribution.

### Lniear Discrimant Analysis

```{r}
class_lda <- lda(Class~., data = traindata2)
class_pred_lda <- predict(class_lda, testdata2)

acc_lda <- mean(class_pred_lda$class == testdata2$Class)
cat("acc:", acc_lda, "\n")
table(class_pred_lda$class, testdata2$Class)

```

### QDA

```{r}
class_qda <- qda(Class~., data = traindata2)

class_pred_qda <- predict(class_qda, testdata2)

acc_qda <- mean(class_pred_qda$class == testdata2$Class)
cat("acc:", acc_qda, "\n")
table(class_pred_qda$class, testdata2$Class)

```

Two classification models, Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA), were then applied to the data. For LDA, the accuracy was 73%, with a confusion matrix showing 45 true negatives, 28 true positives, 11 false positives, and 16 false negatives. For QDA, the accuracy improved to 79%, with a confusion matrix showing 48 true negatives, 31 true positives, 8 false positives, and 13 false negatives.

### Roc

```{r}
#| label: ROC of LDA and QDA
#| fig-cap: ROC 
roc_lda <- roc(testdata2$Class, class_pred_lda$posterior[,2])
plot(roc_lda, main = "ROC for LDA", print.auc = T, auc.polygon = T, legacy.axes = T)

roc_qda <- roc(testdata2$Class, class_pred_qda$posterior[,2])
plot(roc_qda, main = "ROC for QDA", print.auc = T, auc.polygon = T, legacy.axes = T)

```

Finally, ROC curves were plotted for both LDA and QDA. The AUC (Area Under the Curve) values were printed on the plots, providing a visual representation of the models' performance. The AUC for QDA was superior to that of LDA, indicating that QDA had better discriminatory power in distinguishing between the two classes.

### Distribution of Posterior Probabilities

```{r}
#| label: Distribution of Posterior
#| fig-cap: Distribution of Posterior Probabilities 
hist(class_pred_lda$posterior[,2], main = "Distribution of Posterior Probabilities for LDA", xlab = "Probability")
hist(class_pred_qda$posterior[,2], main = "Distribution of Posterior Probabilities for QDA", xlab = "Probability")
```

The histograms show that both LDA and QDA assign most observations with probabilities close to 0 or 1, indicating high confidence in their classifications. This could suggest that the models are performing well, but it might also imply overfitting or sensitivity to the training data.